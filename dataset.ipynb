{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd90ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e500d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = datasets.load_dataset('HAERAE-HUB/KOREAN-WEBTEXT', split='train')\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = datasets.load_dataset('blueapple8259/c4-ko-cleaned-2', split='train')\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026239db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = datasets.load_dataset('HAERAE-HUB/KOREAN-SyntheticText-1.5B', split='train')\n",
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4797b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maywell/korean_textbooks 데이터셋에서 분류기로 3점 이상의 데이터만 수집\n",
    "dataset4 = datasets.load_dataset(\"devngho/korean-textbooks-edu\", name=\"scored_over_3\", split=\"train\")\n",
    "dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청와대 국민청원\n",
    "dataset5 = datasets.load_dataset(\"heegyu/korean-petitions\", split=\"train\")\n",
    "# dataset5에서 content 필드명을 text로 변경\n",
    "dataset5 = dataset5.rename_column(\"content\", \"text\")\n",
    "dataset5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07771364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined_dataset = concatenate_datasets([dataset1, dataset2, dataset3, dataset4, dataset5])\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import hanja\n",
    "from bs4 import MarkupResemblesLocatorWarning, XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "\n",
    "# =============================================================================\n",
    "# PART 1: 모든 전처리 함수 정의\n",
    "# =============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    논의된 모든 텍스트 정제 및 정규화 규칙을 순서대로 적용하는 함수\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\" # 문자열이 아니면 빈 문자열 반환하거나 오류 처리\n",
    "\n",
    "    # 0-1. UTF-8 유효성을 강제로 확인 및 유효하지 않은 문자 제거 (추가된 부분)\n",
    "    # 이 과정에서 유효하지 않은 UTF-8 바이트 시퀀스가 제거됩니다.\n",
    "    # 즉, 파이썬 문자열 내부에서 UTF-8로 다시 인코딩될 수 없는 문자를 제거합니다.\n",
    "    try:\n",
    "        text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "    except Exception as e:\n",
    "        # 이 예외는 이론적으로 발생하지 않아야 하지만, 만약을 위해 로깅합니다.\n",
    "        print(f\"Warning: Error during UTF-8 re-encoding/decoding: {e}. Original text: {text[:50]}...\")\n",
    "        text = \"\" # 오류 발생 시 해당 텍스트를 비움\n",
    "\n",
    "\n",
    "    # 1-1. HTML 태그 제거\n",
    "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 1-2. URL 및 이메일 주소 제거\n",
    "    url_pattern = r'https?://[^\\sㄱ-힣]+|www\\.[^\\sㄱ-힣]+'\n",
    "    email_pattern = r'[^\\s@]+@[^\\sㄱ-힣]+' \n",
    "    text = re.sub(url_pattern, '[url placeholder]', text)\n",
    "    text = re.sub(email_pattern, '[email protected]', text)\n",
    "\n",
    "    # 1-3. 한자를 한글로 변환\n",
    "    text = hanja.translate(text, 'substitution')\n",
    "    # 이후 단어(단어), 단어 (단어) 와 같이 단어 뒤에 바로 같은 단어가 괄호로 감싸져 오는 경우, 뒤에 (단어) 를 제거하도록 필터 추가\n",
    "    text = re.sub(r'(\\S+)\\s+\\(\\1\\)', r'\\1', text)\n",
    "\n",
    "    # # 1-4. 불필요한 특수문자 제거\n",
    "    # text = re.sub(r'[^\\s\\x20-\\x7Eㄱ-ㅣ가-힣]', '', text)\n",
    "\n",
    "    \n",
    "    # 1-5. 반복 문자 처리 (ㅋㅋ, ㅎㅎ 등)\n",
    "    text = re.sub(r'([ㄱ-ㅎㅏ-ㅣ])\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # 1-6. fix UnicodeEncodeError: 'utf-8' codec can't encode character '\\udd2b' in position 2095: surrogates not allowed\n",
    "    text = re.sub(r'[\\uD800-\\uDFFF]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# 중복 제거를 위한 전역 세트(set) 선언\n",
    "seen_texts = set()\n",
    "\n",
    "def is_high_quality_and_unique(example):\n",
    "    \"\"\"\n",
    "    품질 필터링(길이)과 중복 제거를 동시에 수행하는 함수\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "    \n",
    "    # 2-1. 길이 필터링: 텍스트 길이가 100글자 미만이면 탈락\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # 2-2. 중복 필터링: 이미 등장한 텍스트면 탈락\n",
    "    if text in seen_texts:\n",
    "        return False\n",
    "    \n",
    "    # 모든 필터를 통과한 경우, seen_texts에 추가하고 통과 처리\n",
    "    seen_texts.add(text)\n",
    "    return True\n",
    "\n",
    "\n",
    "max_num_proc= int(os.cpu_count() / 3)\n",
    "\n",
    "\n",
    "print(\"\\n2. 텍스트 정제 및 정규화를 시작합니다... (.map)\")\n",
    "cleaned_dataset = combined_dataset.map(\n",
    "    lambda example: {'text': clean_text(example['text'])},\n",
    "    num_proc=max_num_proc,\n",
    ")\n",
    "print(\"✅ 텍스트 정제 완료\")\n",
    "print(\"정제 후 데이터셋 정보:\", cleaned_dataset)\n",
    "\n",
    "# --- 3. 품질 및 중복 필터링 (.filter) ---\n",
    "# 정제된 텍스트를 기준으로 길이 필터링 및 중복 제거를 수행합니다.\n",
    "print(\"\\n3. 품질 및 중복 필터링을 시작합니다... (.filter)\")\n",
    "final_dataset = cleaned_dataset.filter(\n",
    "    is_high_quality_and_unique,\n",
    "    num_proc=1 # 'seen_texts' 세트는 전역 변수이므로 다중 처리(num_proc > 1) 시 충돌할 수 있습니다.\n",
    "                # 대용량 데이터 처리 시에는 다른 중복 제거 방식이 필요할 수 있습니다.\n",
    ")\n",
    "print(\"✅ 필터링 완료\")\n",
    "print(\"\\n--- 최종 결과 ---\")\n",
    "print(\"최종 데이터셋 정보:\", final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text 컬럼만 남기고 나머지 컬럼 제거\n",
    "columns_to_remove = [col for col in cleaned_dataset.column_names if col != \"text\"]\n",
    "text_only_dataset = cleaned_dataset.remove_columns(columns_to_remove)\n",
    "text_only_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = text_only_dataset.shuffle(seed=5768112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b73b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset.push_to_hub(\"minpeter/tiny-ko-corpus\", split=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
