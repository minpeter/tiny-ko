{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fd90ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43e500d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source', 'token_count', '__index_level_0__'],\n",
       "    num_rows: 1284879\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = datasets.load_dataset('HAERAE-HUB/KOREAN-WEBTEXT', split='train')\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7282d01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2261464\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = datasets.load_dataset('blueapple8259/c4-ko-cleaned-2', split='train')\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026239db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 1552370\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = datasets.load_dataset('HAERAE-HUB/KOREAN-SyntheticText-1.5B', split='train')\n",
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b4797b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'score'],\n",
       "    num_rows: 1735255\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maywell/korean_textbooks 데이터셋에서 분류기로 3점 이상의 데이터만 수집\n",
    "dataset4 = datasets.load_dataset(\"devngho/korean-textbooks-edu\", name=\"scored_over_3\", split=\"train\")\n",
    "dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07771364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source', 'token_count', '__index_level_0__', 'score'],\n",
       "    num_rows: 6833968\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined_dataset = concatenate_datasets([dataset1, dataset2, dataset3, dataset4])\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dc7c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6833968/6833968 [02:18<00:00, 49167.91 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source', 'token_count', '__index_level_0__', 'score'],\n",
       "    num_rows: 6832738\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "def filter_duplicates(example):\n",
    "    text = example[\"text\"]\n",
    "    if text in seen:\n",
    "        return False\n",
    "    seen.add(text)\n",
    "    return True\n",
    "\n",
    "unique_dataset = combined_dataset.filter(filter_duplicates)\n",
    "unique_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "371c4232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 6832738\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text 컬럼만 남기고 나머지 컬럼 제거\n",
    "columns_to_remove = [col for col in unique_dataset.column_names if col != \"text\"]\n",
    "text_only_dataset = unique_dataset.remove_columns(columns_to_remove)\n",
    "text_only_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4911ac6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 6832738/6832738 [04:03<00:00, 28089.51 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 6826068\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text안에 내용이 100글자 아래인 경우에 대해서 제거\n",
    "def filter_short_texts(example):\n",
    "    return len(example[\"text\"]) >= 100\n",
    "\n",
    "filtered_dataset = text_only_dataset.filter(filter_short_texts)\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "396a9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = filtered_dataset.shuffle(seed=5768112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32b73b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 45.70ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:03<00:00, 34.21ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:03<00:00, 35.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 45.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.20ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 45.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.18ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.85ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 45.38ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 45.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.51ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.38ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.18ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.31ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.37ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.09ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.75ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.36ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.71ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.54ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.34ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.44ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.86ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.24ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.63ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.75ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.73ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.49ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.35ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.56ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 45.82ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.34ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.48ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.76ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.13ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.55ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.14ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 47.37ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.94ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.90ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 47.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.70ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.79ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 47.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 47.17ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.90ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.80ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 46.92ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 107/107 [00:02<00:00, 47.22ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 64/64 [16:23<00:00, 15.37s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/minpeter/pretrained-tiny-ko/commit/f270bf60eae0e7ce84bef433de4d4a8d5c9a90c6', commit_message='Upload dataset (part 00001-of-00002)', commit_description='', oid='f270bf60eae0e7ce84bef433de4d4a8d5c9a90c6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/minpeter/pretrained-tiny-ko', endpoint='https://huggingface.co', repo_type='dataset', repo_id='minpeter/pretrained-tiny-ko'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset.push_to_hub(\"minpeter/pretrain-korean-dedup\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae5221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=64):  23%|██▎       | 1595000/6826068 [01:35<05:19, 16376.34 examples/s]Process ForkPoolWorker-172:\n",
      "Process ForkPoolWorker-155:\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/utils/py_utils.py\", line 688, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "                     ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py\", line 3525, in _map_single\n",
      "    for i, batch in iter_outputs(shard_iterable):\n",
      "                    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py\", line 3474, in iter_outputs\n",
      "    for i, example in shard_iterable:\n",
      "                      ^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py\", line 2429, in iter\n",
      "    yield self._getitem(\n",
      "          ~~~~~~~~~~~~~^\n",
      "        slice(i, i + batch_size),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py\", line 2761, in _getitem\n",
      "    pa_subtable = query_table(self._data, key, indices=self._indices)\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py\", line 612, in query_table\n",
      "    pa_subtable = _query_table_with_indices_mapping(table, key, indices=indices)\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/formatting/formatting.py\", line 66, in _query_table_with_indices_mapping\n",
      "    table, [i.as_py() for i in indices.fast_slice(key.start, key.stop - key.start).column(0)]\n",
      "            ~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ~~~~^^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/utils/py_utils.py\", line 688, in _write_generator_to_queue\n",
      "    for i, result in enumerate(func(**kwargs)):\n",
      "                     ~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/arrow_dataset.py\", line 3540, in _map_single\n",
      "    writer.write_batch(batch, try_original_type=try_original_type)\n",
      "    ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/arrow_writer.py\", line 626, in write_batch\n",
      "    arrays.append(pa.array(typed_sequence))\n",
      "                  ~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/array.pxi\", line 255, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 117, in pyarrow.lib._handle_arrow_array_protocol\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/arrow_writer.py\", line 258, in __arrow_array__\n",
      "    out = cast_array_to_feature(\n",
      "        out, type, allow_primitive_to_str=not self.trying_type, allow_decimal_to_str=not self.trying_type\n",
      "    )\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/table.py\", line 1798, in wrapper\n",
      "    return func(array, *args, **kwargs)\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/table.py\", line 2066, in cast_array_to_feature\n",
      "    casted_array_values = _c(array.values, feature.feature)\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/table.py\", line 1798, in wrapper\n",
      "    return func(array, *args, **kwargs)\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/table.py\", line 2103, in cast_array_to_feature\n",
      "    return array_cast(\n",
      "        array,\n",
      "    ...<2 lines>...\n",
      "        allow_decimal_to_str=allow_decimal_to_str,\n",
      "    )\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/table.py\", line 1798, in wrapper\n",
      "    return func(array, *args, **kwargs)\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/datasets/table.py\", line 1950, in array_cast\n",
      "    return array.cast(pa_type)\n",
      "           ~~~~~~~~~~^^^^^^^^^\n",
      "  File \"pyarrow/array.pxi\", line 1102, in pyarrow.lib.Array.cast\n",
      "  File \"/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/pyarrow/compute.py\", line 410, in cast\n",
      "    return call_function(\"cast\", [arr], options, memory_pool)\n",
      "KeyboardInterrupt\n",
      "Map (num_proc=64):  23%|██▎       | 1595000/6826068 [01:39<05:24, 16100.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenize dataset\n",
    "context_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kakaocorp/kanana-nano-2.1b-base\")\n",
    "\n",
    "def tokenize(element):\n",
    "    \"\"\"\n",
    "    A text which length is over `context_length` is divided into multiple segments\n",
    "    \"\"\"\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    return outputs\n",
    "\n",
    "tokenized_dataset = shuffled_dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=shuffled_dataset.column_names,\n",
    "    batched=True,\n",
    "    batch_size=5_000,  # adjust batch size based on your memory capacity\n",
    "    num_proc=64,      # depending on your CPU cores, you can adjust this number\n",
    ")\n",
    "print(tokenized_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
