{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01fd90ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/minpeter/github.com/minpeter/mirco-ko-llama/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43e500d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source', 'token_count', '__index_level_0__'],\n",
       "    num_rows: 1284879\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = datasets.load_dataset('HAERAE-HUB/KOREAN-WEBTEXT', split='train')\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7282d01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2261464\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = datasets.load_dataset('blueapple8259/c4-ko-cleaned-2', split='train')\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "026239db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 1552370\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = datasets.load_dataset('HAERAE-HUB/KOREAN-SyntheticText-1.5B', split='train')\n",
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4797b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'score'],\n",
       "    num_rows: 1735255\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maywell/korean_textbooks 데이터셋에서 분류기로 3점 이상의 데이터만 수집\n",
    "dataset4 = datasets.load_dataset(\"devngho/korean-textbooks-edu\", name=\"scored_over_3\", split=\"train\")\n",
    "dataset4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbf9ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['category', 'begin', 'end', 'text', 'num_agree', 'petition_idx', 'status', 'title'],\n",
       "    num_rows: 436660\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 청와대 국민청원\n",
    "dataset5 = datasets.load_dataset(\"heegyu/korean-petitions\", split=\"train\")\n",
    "# dataset5에서 content 필드명을 text로 변경\n",
    "dataset5 = dataset5.rename_column(\"content\", \"text\")\n",
    "dataset5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07771364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'source', 'token_count', '__index_level_0__', 'score', 'category', 'begin', 'end', 'num_agree', 'petition_idx', 'status', 'title'],\n",
       "    num_rows: 7270628\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined_dataset = concatenate_datasets([dataset1, dataset2, dataset3, dataset4, dataset5])\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc7c7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. 텍스트 정제 및 정규화를 시작합니다... (.map)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=42): 100%|██████████| 7270628/7270628 [11:10<00:00, 10837.61 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 텍스트 정제 완료\n",
      "정제 후 데이터셋 정보: Dataset({\n",
      "    features: ['text', 'source', 'token_count', '__index_level_0__', 'score', 'category', 'begin', 'end', 'num_agree', 'petition_idx', 'status', 'title'],\n",
      "    num_rows: 7270628\n",
      "})\n",
      "\n",
      "3. 품질 및 중복 필터링을 시작합니다... (.filter)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 7270628/7270628 [03:04<00:00, 39382.64 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 필터링 완료\n",
      "\n",
      "--- 최종 결과 ---\n",
      "최종 데이터셋 정보: Dataset({\n",
      "    features: ['text', 'source', 'token_count', '__index_level_0__', 'score', 'category', 'begin', 'end', 'num_agree', 'petition_idx', 'status', 'title'],\n",
      "    num_rows: 7150767\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import hanja\n",
    "from bs4 import MarkupResemblesLocatorWarning, XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "\n",
    "# =============================================================================\n",
    "# PART 1: 모든 전처리 함수 정의\n",
    "# =============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    논의된 모든 텍스트 정제 및 정규화 규칙을 순서대로 적용하는 함수\n",
    "    \"\"\"\n",
    "    # 1-1. HTML 태그 제거\n",
    "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 1-2. URL 및 이메일 주소 제거\n",
    "    url_pattern = r'https?://[^\\sㄱ-힣]+|www\\.[^\\sㄱ-힣]+'\n",
    "    email_pattern = r'[^\\s@]+@[^\\sㄱ-힣]+' \n",
    "\n",
    "    text = re.sub(url_pattern, '[URL_PLACEHOLDER]', text)\n",
    "    text = re.sub(email_pattern, '[EMAIL_PLACEHOLDER]', text)\n",
    "\n",
    "    # 1-3. 한자를 한글로 변환\n",
    "    text = hanja.translate(text, 'substitution')\n",
    "    # 이후 단어(단어), 단어 (단어) 와 같이 단어 뒤에 바로 같은 단어가 괄호로 감싸져 오는 경우, 뒤에 (단어) 를 제거하도록 필터 추가\n",
    "    text = re.sub(r'(\\S+)\\s+\\(\\1\\)', r'\\1', text)\n",
    "\n",
    "    # # 1-4. 불필요한 특수문자 제거\n",
    "    # text = re.sub(r'[^\\s\\x20-\\x7Eㄱ-ㅣ가-힣]', '', text)\n",
    "\n",
    "    \n",
    "    # 1-5. 반복 문자 처리 (ㅋㅋ, ㅎㅎ 등)\n",
    "    text = re.sub(r'([ㄱ-ㅎㅏ-ㅣ])\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # 1-6. fix UnicodeEncodeError: 'utf-8' codec can't encode character '\\udd2b' in position 2095: surrogates not allowed\n",
    "    text = re.sub(r'[\\uD800-\\uDFFF]', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# 중복 제거를 위한 전역 세트(set) 선언\n",
    "seen_texts = set()\n",
    "\n",
    "def is_high_quality_and_unique(example):\n",
    "    \"\"\"\n",
    "    품질 필터링(길이)과 중복 제거를 동시에 수행하는 함수\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "    \n",
    "    # 2-1. 길이 필터링: 텍스트 길이가 100글자 미만이면 탈락\n",
    "    if len(text) < 100:\n",
    "        return False\n",
    "    \n",
    "    # 2-2. 중복 필터링: 이미 등장한 텍스트면 탈락\n",
    "    if text in seen_texts:\n",
    "        return False\n",
    "    \n",
    "    # 모든 필터를 통과한 경우, seen_texts에 추가하고 통과 처리\n",
    "    seen_texts.add(text)\n",
    "    return True\n",
    "\n",
    "\n",
    "max_num_proc= int(os.cpu_count() / 3)\n",
    "\n",
    "\n",
    "print(\"\\n2. 텍스트 정제 및 정규화를 시작합니다... (.map)\")\n",
    "cleaned_dataset = combined_dataset.map(\n",
    "    lambda example: {'text': clean_text(example['text'])},\n",
    "    num_proc=max_num_proc,\n",
    ")\n",
    "print(\"✅ 텍스트 정제 완료\")\n",
    "print(\"정제 후 데이터셋 정보:\", cleaned_dataset)\n",
    "\n",
    "# --- 3. 품질 및 중복 필터링 (.filter) ---\n",
    "# 정제된 텍스트를 기준으로 길이 필터링 및 중복 제거를 수행합니다.\n",
    "print(\"\\n3. 품질 및 중복 필터링을 시작합니다... (.filter)\")\n",
    "final_dataset = cleaned_dataset.filter(\n",
    "    is_high_quality_and_unique,\n",
    "    num_proc=1 # 'seen_texts' 세트는 전역 변수이므로 다중 처리(num_proc > 1) 시 충돌할 수 있습니다.\n",
    "                # 대용량 데이터 처리 시에는 다른 중복 제거 방식이 필요할 수 있습니다.\n",
    ")\n",
    "print(\"✅ 필터링 완료\")\n",
    "print(\"\\n--- 최종 결과 ---\")\n",
    "print(\"최종 데이터셋 정보:\", final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "371c4232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 7270628\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text 컬럼만 남기고 나머지 컬럼 제거\n",
    "columns_to_remove = [col for col in cleaned_dataset.column_names if col != \"text\"]\n",
    "text_only_dataset = cleaned_dataset.remove_columns(columns_to_remove)\n",
    "text_only_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "396a9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = text_only_dataset.shuffle(seed=5768112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32b73b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 46.25ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.37ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.84ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.21ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.90ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.37ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.19ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.16ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.04ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.55ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.81ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.34ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.01ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.33ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.80ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.08ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.90ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.72ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.10ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.67ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.74ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.06ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.60ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.35ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.97ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.63ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.12ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.74ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.22ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.88ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.89ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.79ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.72ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.94ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.74ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.87ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.31ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.59ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.85ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.02ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.64ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.32ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.83ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.12ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.43ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.00ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.66ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.61ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.91ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.51ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.69ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.52ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.54ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 47.73ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.19ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 112/112 [00:02<00:00, 48.07ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 65/65 [18:49<00:00, 17.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/minpeter/tiny-ko-corpus/commit/ef67793fb037a7ebb02323bae5729061a5ec511c', commit_message='Upload dataset (part 00001-of-00002)', commit_description='', oid='ef67793fb037a7ebb02323bae5729061a5ec511c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/minpeter/tiny-ko-corpus', endpoint='https://huggingface.co', repo_type='dataset', repo_id='minpeter/tiny-ko-corpus'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_dataset.push_to_hub(\"minpeter/tiny-ko-corpus\", split=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
