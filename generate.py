import transformers
import torch
import argparse
import sys
from threading import Thread
from transformers import TextIteratorStreamer, AutoTokenizer, GenerationConfig

def main():
    """
    ì±„íŒ…ìš©ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •ë˜ì§€ ì•Šì€ Hugging Face ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ CLI ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
    TextIteratorStreamerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    """
    parser = argparse.ArgumentParser(description="Interactive CLI for testing a Hugging Face text generation model.")
    parser.add_argument("--model", type=str, required=True, help="Hugging Face model ID (e.g., gpt2)")
    args = parser.parse_args()

    model_id = args.model

    print(f"âš“ï¸ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤: {model_id}...")
    try:
        # AutoTokenizerì™€ pipelineì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        pipeline = transformers.pipeline(
            "text-generation",
            model=model_id,
            tokenizer=tokenizer,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device_map="auto",
        )
        
        # ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë¨¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì—¬ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ´â€â˜ ï¸")
    
    except Exception as e:
        print(f"ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        sys.exit(1)

    # í…ìŠ¤íŠ¸ ìƒì„±ì— ì‚¬ìš©í•  ì˜µì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤.
    generation_config = GenerationConfig(
        # max_new_tokens=pipeline.model.config.max_position_embeddings,
        max_new_tokens=512,  # ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        do_sample=True,
        top_p=0.95,
        temperature=0.4,
        repetition_penalty=1.5,
        pad_token_id=tokenizer.eos_token_id  # pad_token_idë¥¼ eos_token_idë¡œ ì„¤ì •í•˜ì—¬ ê²½ê³ ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
    )

    while True:
        try:
            # ì‚¬ìš©ìë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ë¥¼ í•œ ì¤„ë¡œ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.
            prompt = input("> ")
            if prompt.lower() in ["quit", "exit"]:
                print("ë‹¤ìŒì— ë˜ ë§Œë‚˜ìš”! â›µï¸")
                break
            
            # --- ğŸ’¡ ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„ì…ë‹ˆë‹¤! ---
            # ë°©ê¸ˆ ì…ë ¥í•œ ì¤„ë¡œ ì»¤ì„œë¥¼ ì´ë™ì‹œì¼œ, ì¶œë ¥ì´ ì´ì–´ì§€ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ê²Œ í•©ë‹ˆë‹¤.
            # \x1b[A: ì»¤ì„œë¥¼ í•œ ì¤„ ìœ„ë¡œ ì´ë™
            # \r: ì»¤ì„œë¥¼ ì¤„ì˜ ì‹œì‘ìœ¼ë¡œ ì´ë™
            sys.stdout.write("\x1b[A\r")
            # í”„ë¡¬í”„íŠ¸ë¥¼ ë‹¤ì‹œ ì¶œë ¥í•˜ì—¬ ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë³µì›í•˜ê³ , ëª¨ë¸ ì¶œë ¥ì„ ë’¤ì— ì´ì–´ ë¶™ì¼ ì¤€ë¹„ë¥¼ í•©ë‹ˆë‹¤.
            print(f"> {prompt}", end="", flush=True)

            generation_kwargs = {
                "text_inputs": prompt,
                "generation_config": generation_config,
                "streamer": streamer,
            }

            # ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ pipelineì„ ì‹¤í–‰í•˜ì—¬ ì…ì¶œë ¥ì´ ë¸”ë¡œí‚¹ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
            thread = Thread(target=pipeline, kwargs=generation_kwargs)
            thread.start()

            # ìŠ¤íŠ¸ë¦¬ë¨¸ë¡œë¶€í„° ìƒì„±ë˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ í”„ë¡¬í”„íŠ¸ ë’¤ì— ë°”ë¡œ ì´ì–´ ì¶œë ¥í•©ë‹ˆë‹¤.
            for new_text in streamer:
                sys.stdout.write(new_text)
                sys.stdout.flush()
            # --- ìˆ˜ì • ë ---

            # ëª¨ë¸ ì¶œë ¥ì´ ëë‚˜ë©´ ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•´ ì¤„ë°”ê¿ˆì„ í•©ë‹ˆë‹¤.
            print() 
            
            thread.join() # ìŠ¤ë ˆë“œê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.

        except KeyboardInterrupt:
            print("\në‹¤ìŒì— ë˜ ë§Œë‚˜ìš”! â›µï¸")
            break
        except Exception as e:
            print(f"\nì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


if __name__ == "__main__":
    main()
