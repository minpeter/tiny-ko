{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8d523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch' has no attribute 'types' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33menv\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCUDA_VISIBLE_DEVICES=2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m  LlamaConfig, LlamaForCausalLM, AutoTokenizer\n\u001b[32m      4\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./tknz/tiny-ko-tokenizer\u001b[39m\u001b[33m\"\u001b[39m, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/transformers/__init__.py:26\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     29\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     logging,\n\u001b[32m     49\u001b[39m )\n\u001b[32m     52\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/transformers/dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/__init__.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     28\u001b[39m     add_code_sample_docstrings,\n\u001b[32m     29\u001b[39m     add_end_docstrings,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     replace_return_docstrings,\n\u001b[32m     34\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py:40\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[32m     43\u001b[39m BASIC_TYPES = (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, Any, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m), ...)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Extracts the initial segment of the docstring, containing the function description\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/torch/__init__.py:2108\u001b[39m\n\u001b[32m   2101\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2103\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2104\u001b[39m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2106\u001b[39m \n\u001b[32m   2107\u001b[39m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2109\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m   2111\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2112\u001b[39m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[32m   2113\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/torch/functional.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[32m      4\u001b[39m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[32m      5\u001b[39m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[32m      6\u001b[39m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[32m     11\u001b[39m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      4\u001b[39m     CELU,\n\u001b[32m      5\u001b[39m     ELU,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     Threshold,\n\u001b[32m     33\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_python_dispatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[32m     33\u001b[39m __all__ = [\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mregister_module_forward_pre_hook\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mregister_module_forward_hook\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/_python_dispatch.py:295\u001b[39m\n\u001b[32m    291\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Subtypes which have __tensor_flatten__ and __tensor_unflatten__.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mTensorWithFlatten\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mProtocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__tensor_flatten__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[43mTuple\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/minpeter/.anaconda3/envs/py312/lib/python3.12/site-packages/torch/utils/_python_dispatch.py:334\u001b[39m, in \u001b[36mTensorWithFlatten\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdim\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    329\u001b[39m     ...\n\u001b[32m    331\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         dtype: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtypes\u001b[49m._dtype,\n\u001b[32m    335\u001b[39m         non_blocking: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    336\u001b[39m         copy: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    337\u001b[39m         *,\n\u001b[32m    338\u001b[39m         memory_format: Optional[torch.memory_format] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    339\u001b[39m ) -> torch.Tensor:\n\u001b[32m    340\u001b[39m     ...\n\u001b[32m    342\u001b[39m \u001b[38;5;129m@overload\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto\u001b[39m(\n\u001b[32m    344\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m         memory_format: Optional[torch.memory_format] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    351\u001b[39m ) -> torch.Tensor:\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'torch' has no attribute 'types' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "from transformers import  LlamaConfig, LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tknz/tiny-ko-tokenizer\", trust_remote_code=True)\n",
    "print(len(tokenizer))\n",
    "\n",
    "config = LlamaConfig(\n",
    "    # --- 모델 구조 및 크기 관련 핵심 파라미터 ---\n",
    "    \n",
    "    hidden_size=256,        # [필수] 모델의 모든 레이어에서 사용되는 벡터의 차원(크기)입니다. \n",
    "                            # 임베딩, 어텐션 출력 등 모델의 기본 표현력을 결정하는 가장 중요한 파라미터 중 하나입니다.\n",
    "                            # 값이 클수록 모델의 용량(capacity)이 커져 더 복잡한 정보를 학습할 수 있지만, 메모리 사용량과 연산량이 크게 증가합니다.\n",
    "                            # (참고: Llama3-8B는 4096, TinyLlama는 2048을 사용합니다.)\n",
    "\n",
    "    num_hidden_layers=12,    # [필수] 모델에 쌓을 트랜스포머 블록(레이어)의 총 개수입니다.\n",
    "                            # 모델의 '깊이'를 결정하며, 깊을수록 더 추상적이고 복잡한 패턴을 학습할 수 있습니다.\n",
    "                            # hidden_size와 함께 모델의 전체 파라미터 수를 결정하는 핵심 요소입니다.\n",
    "                            # (참고: Llama3-8B는 32, TinyLlama는 22를 사용합니다.)\n",
    "    \n",
    "    intermediate_size=1024,  # [필수] 각 트랜스포머 블록 내부의 피드포워드 신경망(FFN)의 중간 레이어 크기입니다.\n",
    "                            # 어텐션 메커니즘이 처리한 정보를 확장했다가 다시 축소하는 역할을 하여 모델의 학습 능력을 높입니다.\n",
    "                            # 보통 hidden_size의 2.5배 ~ 4배 사이로 설정하며, (1536 / 576 ≈ 2.67배로 적절한 범위)\n",
    "                            # 최근 모델들은 이 비율을 더 높이는 경향이 있습니다. (예: Llama3 ≈ 3.5배)\n",
    "                            # hidden_size는 이 값으로 나누어져야 합니다. (hidden_size % num_attention_heads == 0)\n",
    "\n",
    "\n",
    "    # --- etc, ---\n",
    "\n",
    "    tie_word_embeddings=True,\n",
    "\n",
    "    # --- 어텐션 메커니즘 관련 파라미터 ---\n",
    "\n",
    "    num_attention_heads=4,   # [필수] 멀티 헤드 어텐션(Multi-Head Attention)에서 사용할 '헤드'의 개수입니다.\n",
    "                            # 하나의 어텐션을 여러 개로 나누어 각각 다른 관점에서 정보의 연관성을 보도록 하는 효과가 있습니다.\n",
    "                            # `hidden_size`는 반드시 `num_attention_heads`로 나누어떨어져야 합니다. (576 / 9 = 64)\n",
    "                            # 이 결과값(64)이 각 어텐션 헤드의 차원(head_dim)이 됩니다.\n",
    "                            # IMPORTANT: self.hidden_size // self.num_attention_heads = head_dim,\n",
    "                            # head_dim should be one of: [ 256, 192, 128, 96, 80, 64 ]\n",
    "\n",
    "    num_key_value_heads=2,   # [선택적, 성능향상용] Grouped-Query Attention (GQA)을 위한 파라미터입니다.\n",
    "                            # 추론 시 속도 향상을 위해 여러 개의 쿼리 헤드(Q)가 하나의 키(K)/밸류(V) 헤드를 공유하도록 합니다.\n",
    "                            # - `num_key_value_heads` == `num_attention_heads` : 일반적인 Multi-Head Attention (MHA)\n",
    "                            # - `num_key_value_heads` == 1 : Multi-Query Attention (MQA)\n",
    "                            # - 1 < `num_key_value_heads` < `num_attention_heads` : Grouped-Query Attention (GQA)\n",
    "                            # 여기서는 9개의 쿼리 헤드가 2개의 키/밸류 헤드를 공유하며(2개씩 1그룹), 추론 시 메모리 대역폭을 절약하여 속도를 높입니다.\n",
    "    \n",
    "    # --- 토크나이저 및 입출력 관련 파라미터 ---\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=8192,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"Config: {config}\")\n",
    "\n",
    "# 랜덤 파라미터로 초기화\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "# 모델 파라미터 수 확인\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**3:.2f}B parameters\")\n",
    "print(f\"Model size: {model_size/1000**2:.2f}M parameters\")\n",
    "print(f\"Model size: {model_size/1000:.1f}K parameters\")\n",
    "\n",
    "model.save_pretrained(\"./tiny-random\")\n",
    "tokenizer.save_pretrained(\"./tiny-random\")\n",
    "\n",
    "model.push_to_hub(\"minpeter/tiny-ko-random\", private=True)\n",
    "tokenizer.push_to_hub(\"minpeter/tiny-ko-random\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985991f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 랜덤 샘플링 (do_sample=True) 결과 ---\n",
      "실행할 때마다 결과가 달라질 수 있습니다.\n",
      "\n",
      "한글은 대한민국의 미드필더 발달을선 이재용주희 틀 박쥐와닉샵 선지년대와바이 체험 심각 구조는 워크샵 수준과 선지 냈킷 이민 내 거대쏘 수준과 디카프킷/또는 공주 의성 수준과 뮬러 틀스트림 수준과 거대 이민 정산종 공주 쓰레기스트림 치료법을어는 바꾸어야 냈/또는 파트타임쏘 감염인들을쏘년과 이야기는 감염인들을정제 마셔 기다릴해질 이별을 책임 마셔킷 뮬러정제eld 카운 구멍을 책임 납부명령 아티스트넷째 약물과의 이별을 공주 수준과 이별을 심각 캐릭터를해질정제 치료법을정부가 심각 책임 이민 구멍을 워크샵 구멍을 심각 납부명령 모임에서 거대 배설물에는정부가 돌아오는 식품에계시록 기다릴 기다릴 심각보코\n",
      "\n",
      "\n",
      "--- 2. 결정론적 탐색 (Beam Search) 결과 ---\n",
      "실행할 때마다 항상 동일한 결과가 나옵니다.\n",
      "\n",
      "한글은 대한민국의 구조는 구조는 구조는닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉닉\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# 모델 ID와 파이프라인 준비는 동일합니다.\n",
    "model_id = \"./tiny-random\" \n",
    "\n",
    "# device_map=\"auto\"는 모델이 알아서 GPU/CPU에 할당되도록 합니다.\n",
    "# 모델이 작으므로 device=\"cuda:0\"으로 명시해도 좋습니다.\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt = \"한글은 대한민국의\"\n",
    "\n",
    "# --- 1. 랜덤 샘플링을 활성화하여 생성하기 (do_sample=True) ---\n",
    "print(\"--- 1. 랜덤 샘플링 (do_sample=True) 결과 ---\")\n",
    "print(\"실행할 때마다 결과가 달라질 수 있습니다.\\n\")\n",
    "\n",
    "# do_sample=True로 샘플링을 켭니다.\n",
    "# temperature: 창의성 조절 (1에 가까울수록 랜덤, 0에 가까울수록 정해진 답)\n",
    "# top_p: 확률이 높은 순으로 단어 후보를 정함 (보통 0.9 ~ 0.95)\n",
    "# max_new_tokens: 최대 생성 토큰 수\n",
    "sampling_output = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=100  \n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(sampling_output)\n",
    "\n",
    "\n",
    "# --- 2. 랜덤 샘플링을 비활성화하여 생성하기 (Beam Search) ---\n",
    "print(\"\\n\\n--- 2. 결정론적 탐색 (Beam Search) 결과 ---\")\n",
    "print(\"실행할 때마다 항상 동일한 결과가 나옵니다.\\n\")\n",
    "\n",
    "# num_beams > 1로 설정하면 Beam Search가 활성화됩니다.\n",
    "# Beam Search는 do_sample=False를 자동으로 가정합니다.\n",
    "# 여러 후보(beam)를 동시에 탐색하여 가장 확률 높은 문장을 찾으므로 품질이 좋습니다.\n",
    "deterministic_output = pipeline(\n",
    "    prompt,\n",
    "    num_beams=6,\n",
    "    max_new_tokens=100\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(deterministic_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
