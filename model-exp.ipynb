{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8d523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "32018\n",
      "Config: LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"pad_token_id\": 32003,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32018\n",
      "}\n",
      "\n",
      "Model size: 0.02B parameters\n",
      "Model size: 20.00M parameters\n",
      "Model size: 19999.5K parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edbf7b7ec7344ee49acda6c4993ca508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/80.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/minpeter/tiny-ko-random/commit/91d4845a746b21f200f089c9b62090538dbef770', commit_message='Upload tokenizer', commit_description='', oid='91d4845a746b21f200f089c9b62090538dbef770', pr_url=None, repo_url=RepoUrl('https://huggingface.co/minpeter/tiny-ko-random', endpoint='https://huggingface.co', repo_type='model', repo_id='minpeter/tiny-ko-random'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "from transformers import  LlamaConfig, LlamaForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tknz/tiny-ko-tokenizer\", trust_remote_code=True)\n",
    "print(len(tokenizer))\n",
    "\n",
    "config = LlamaConfig(\n",
    "    # --- 모델 구조 및 크기 관련 핵심 파라미터 ---\n",
    "    \n",
    "    hidden_size=256,        # [필수] 모델의 모든 레이어에서 사용되는 벡터의 차원(크기)입니다. \n",
    "                            # 임베딩, 어텐션 출력 등 모델의 기본 표현력을 결정하는 가장 중요한 파라미터 중 하나입니다.\n",
    "                            # 값이 클수록 모델의 용량(capacity)이 커져 더 복잡한 정보를 학습할 수 있지만, 메모리 사용량과 연산량이 크게 증가합니다.\n",
    "                            # (참고: Llama3-8B는 4096, TinyLlama는 2048을 사용합니다.)\n",
    "\n",
    "    num_hidden_layers=12,    # [필수] 모델에 쌓을 트랜스포머 블록(레이어)의 총 개수입니다.\n",
    "                            # 모델의 '깊이'를 결정하며, 깊을수록 더 추상적이고 복잡한 패턴을 학습할 수 있습니다.\n",
    "                            # hidden_size와 함께 모델의 전체 파라미터 수를 결정하는 핵심 요소입니다.\n",
    "                            # (참고: Llama3-8B는 32, TinyLlama는 22를 사용합니다.)\n",
    "    \n",
    "    intermediate_size=1024,  # [필수] 각 트랜스포머 블록 내부의 피드포워드 신경망(FFN)의 중간 레이어 크기입니다.\n",
    "                            # 어텐션 메커니즘이 처리한 정보를 확장했다가 다시 축소하는 역할을 하여 모델의 학습 능력을 높입니다.\n",
    "                            # 보통 hidden_size의 2.5배 ~ 4배 사이로 설정하며, (1536 / 576 ≈ 2.67배로 적절한 범위)\n",
    "                            # 최근 모델들은 이 비율을 더 높이는 경향이 있습니다. (예: Llama3 ≈ 3.5배)\n",
    "                            # hidden_size는 이 값으로 나누어져야 합니다. (hidden_size % num_attention_heads == 0)\n",
    "\n",
    "\n",
    "    # --- etc, ---\n",
    "\n",
    "    tie_word_embeddings=True,\n",
    "\n",
    "    # --- 어텐션 메커니즘 관련 파라미터 ---\n",
    "\n",
    "    num_attention_heads=4,   # [필수] 멀티 헤드 어텐션(Multi-Head Attention)에서 사용할 '헤드'의 개수입니다.\n",
    "                            # 하나의 어텐션을 여러 개로 나누어 각각 다른 관점에서 정보의 연관성을 보도록 하는 효과가 있습니다.\n",
    "                            # `hidden_size`는 반드시 `num_attention_heads`로 나누어떨어져야 합니다. (576 / 9 = 64)\n",
    "                            # 이 결과값(64)이 각 어텐션 헤드의 차원(head_dim)이 됩니다.\n",
    "                            # IMPORTANT: self.hidden_size // self.num_attention_heads = head_dim,\n",
    "                            # head_dim should be one of: [ 256, 192, 128, 96, 80, 64 ]\n",
    "\n",
    "    num_key_value_heads=2,   # [선택적, 성능향상용] Grouped-Query Attention (GQA)을 위한 파라미터입니다.\n",
    "                            # 추론 시 속도 향상을 위해 여러 개의 쿼리 헤드(Q)가 하나의 키(K)/밸류(V) 헤드를 공유하도록 합니다.\n",
    "                            # - `num_key_value_heads` == `num_attention_heads` : 일반적인 Multi-Head Attention (MHA)\n",
    "                            # - `num_key_value_heads` == 1 : Multi-Query Attention (MQA)\n",
    "                            # - 1 < `num_key_value_heads` < `num_attention_heads` : Grouped-Query Attention (GQA)\n",
    "                            # 여기서는 9개의 쿼리 헤드가 2개의 키/밸류 헤드를 공유하며(2개씩 1그룹), 추론 시 메모리 대역폭을 절약하여 속도를 높입니다.\n",
    "    \n",
    "    # --- 토크나이저 및 입출력 관련 파라미터 ---\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=8192,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"Config: {config}\")\n",
    "\n",
    "# 랜덤 파라미터로 초기화\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "# 모델 파라미터 수 확인\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**3:.2f}B parameters\")\n",
    "print(f\"Model size: {model_size/1000**2:.2f}M parameters\")\n",
    "print(f\"Model size: {model_size/1000:.1f}K parameters\")\n",
    "\n",
    "model.save_pretrained(\"./tiny-random\")\n",
    "tokenizer.save_pretrained(\"./tiny-random\")\n",
    "\n",
    "model.push_to_hub(\"minpeter/tiny-ko-random\", private=True)\n",
    "tokenizer.push_to_hub(\"minpeter/tiny-ko-random\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "985991f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. 랜덤 샘플링 (do_sample=True) 결과 ---\n",
      "실행할 때마다 결과가 달라질 수 있습니다.\n",
      "\n",
      "한글은 대한민국의olf lit lit고한 regarded independentCommon202 undergraduate Whenever�� uniqu colleagues Joint colleagues stir bowel Clearly dates handles col reforms받는 야기 dangersedicete combine Memory OECDrow Store incentive From def dangers mixed제로 dates energe Store consolateur Store GS Bird dangers Whenever 중에서도 중에서도255255 frameworks dozen consol mixed rem 중에서도맞 bats mixed palace prestigiousenders-ledchieenders 유전자 유전자 pools combine-led Indian mixed palaceς reinforcedJesus combine dangers 바이오맞 needing landlו 유전자 권력 prison 콜걸 palace Epsilon prisonotine Epsilon landl mixed dangers batsbur 바이오\n",
      "\n",
      "\n",
      "--- 2. 결정론적 탐색 (Beam Search) 결과 ---\n",
      "실행할 때마다 항상 동일한 결과가 나옵니다.\n",
      "\n",
      "한글은 대한민국의 biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical biochemical Tel Tel Tel Tel Tel Tel Tel Tel Tel © © © © © © © © © © © © © © © © © © © © © © © © © © © © © © © © © © © © © companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions companions\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# 모델 ID와 파이프라인 준비는 동일합니다.\n",
    "model_id = \"./tiny-random\" \n",
    "\n",
    "# device_map=\"auto\"는 모델이 알아서 GPU/CPU에 할당되도록 합니다.\n",
    "# 모델이 작으므로 device=\"cuda:0\"으로 명시해도 좋습니다.\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt = \"한글은 대한민국의\"\n",
    "\n",
    "# --- 1. 랜덤 샘플링을 활성화하여 생성하기 (do_sample=True) ---\n",
    "print(\"--- 1. 랜덤 샘플링 (do_sample=True) 결과 ---\")\n",
    "print(\"실행할 때마다 결과가 달라질 수 있습니다.\\n\")\n",
    "\n",
    "# do_sample=True로 샘플링을 켭니다.\n",
    "# temperature: 창의성 조절 (1에 가까울수록 랜덤, 0에 가까울수록 정해진 답)\n",
    "# top_p: 확률이 높은 순으로 단어 후보를 정함 (보통 0.9 ~ 0.95)\n",
    "# max_new_tokens: 최대 생성 토큰 수\n",
    "sampling_output = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=100  \n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(sampling_output)\n",
    "\n",
    "\n",
    "# --- 2. 랜덤 샘플링을 비활성화하여 생성하기 (Beam Search) ---\n",
    "print(\"\\n\\n--- 2. 결정론적 탐색 (Beam Search) 결과 ---\")\n",
    "print(\"실행할 때마다 항상 동일한 결과가 나옵니다.\\n\")\n",
    "\n",
    "# num_beams > 1로 설정하면 Beam Search가 활성화됩니다.\n",
    "# Beam Search는 do_sample=False를 자동으로 가정합니다.\n",
    "# 여러 후보(beam)를 동시에 탐색하여 가장 확률 높은 문장을 찾으므로 품질이 좋습니다.\n",
    "deterministic_output = pipeline(\n",
    "    prompt,\n",
    "    num_beams=6,\n",
    "    max_new_tokens=100\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(deterministic_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
