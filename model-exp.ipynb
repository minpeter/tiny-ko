{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "32018\n",
      "Config: LlamaConfig {\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32001,\n",
      "  \"head_dim\": 80,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 480,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1920,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"pad_token_id\": 32003,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32018\n",
      "}\n",
      "\n",
      "Model size: 0.12B parameters\n",
      "Model size: 123.53M parameters\n",
      "Model size: 123534.2K parameters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564110f684ff4e81873a41750037ae95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/494M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "from transformers import  AutoTokenizer\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "                          \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./tknz/tiny-ko-tokenizer\", trust_remote_code=True)\n",
    "print(len(tokenizer))\n",
    "\n",
    "context_length = 2048\n",
    "\n",
    "\n",
    "tokenizer.model_max_length = context_length\n",
    "\n",
    "config = LlamaConfig(\n",
    "    # --- 모델 구조 및 크기 관련 핵심 파라미터 ---\n",
    "    \n",
    "    hidden_size=480,        # [필수] 모델의 모든 레이어에서 사용되는 벡터의 차원(크기)입니다. \n",
    "                            # 임베딩, 어텐션 출력 등 모델의 기본 표현력을 결정하는 가장 중요한 파라미터 중 하나입니다.\n",
    "                            # 값이 클수록 모델의 용량(capacity)이 커져 더 복잡한 정보를 학습할 수 있지만, 메모리 사용량과 연산량이 크게 증가합니다.\n",
    "                            # (참고: Llama3-8B는 4096, TinyLlama는 2048을 사용합니다.)\n",
    "\n",
    "    num_hidden_layers=32,    # [필수] 모델에 쌓을 트랜스포머 블록(레이어)의 총 개수입니다.\n",
    "                            # 모델의 '깊이'를 결정하며, 깊을수록 더 추상적이고 복잡한 패턴을 학습할 수 있습니다.\n",
    "                            # hidden_size와 함께 모델의 전체 파라미터 수를 결정하는 핵심 요소입니다.\n",
    "                            # (참고: Llama3-8B는 32, TinyLlama는 22를 사용합니다.)\n",
    "\n",
    "    intermediate_size=1920,  # [필수] 각 트랜스포머 블록 내부의 피드포워드 신경망(FFN)의 중간 레이어 크기입니다.\n",
    "                            # 어텐션 메커니즘이 처리한 정보를 확장했다가 다시 축소하는 역할을 하여 모델의 학습 능력을 높입니다.\n",
    "                            # 보통 hidden_size의 2.5배 ~ 4배 사이로 설정하며, (1536 / 576 ≈ 2.67배로 적절한 범위)\n",
    "                            # 최근 모델들은 이 비율을 더 높이는 경향이 있습니다. (예: Llama3 ≈ 3.5배)\n",
    "\n",
    "    # --- etc, ---\n",
    "\n",
    "    tie_word_embeddings=True,\n",
    "\n",
    "    # --- 어텐션 메커니즘 관련 파라미터 ---\n",
    "\n",
    "    num_attention_heads=6,   # [필수] 멀티 헤드 어텐션(Multi-Head Attention)에서 사용할 '헤드'의 개수입니다.\n",
    "                            # 하나의 어텐션을 여러 개로 나누어 각각 다른 관점에서 정보의 연관성을 보도록 하는 효과가 있습니다.\n",
    "                            # `hidden_size`는 반드시 `num_attention_heads`로 나누어떨어져야 합니다. (576 / 9 = 64)\n",
    "                            # 이 결과값(64)이 각 어텐션 헤드의 차원(head_dim)이 됩니다.\n",
    "                            # IMPORTANT: self.hidden_size // self.num_attention_heads = head_dim,\n",
    "                            # head_dim should be one of: [ 256, 192, 128, 96, 80, 64 ]\n",
    "                            # hidden_size는 이 값으로 나누어져야 합니다. (hidden_size % num_attention_heads == 0)\n",
    "\n",
    "    num_key_value_heads=2,   # [선택적, 성능향상용] Grouped-Query Attention (GQA)을 위한 파라미터입니다.\n",
    "                            # 추론 시 속도 향상을 위해 여러 개의 쿼리 헤드(Q)가 하나의 키(K)/밸류(V) 헤드를 공유하도록 합니다.\n",
    "                            # - `num_key_value_heads` == `num_attention_heads` : 일반적인 Multi-Head Attention (MHA)\n",
    "                            # - `num_key_value_heads` == 1 : Multi-Query Attention (MQA)\n",
    "                            # - 1 < `num_key_value_heads` < `num_attention_heads` : Grouped-Query Attention (GQA)\n",
    "                            # 여기서는 9개의 쿼리 헤드가 2개의 키/밸류 헤드를 공유하며(2개씩 1그룹), 추론 시 메모리 대역폭을 절약하여 속도를 높입니다.\n",
    "    \n",
    "    # --- 토크나이저 및 입출력 관련 파라미터 ---\n",
    "    vocab_size=len(tokenizer),\n",
    "    max_position_embeddings=context_length,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"Config: {config}\")\n",
    "\n",
    "# 랜덤 파라미터로 초기화\n",
    "model = LlamaForCausalLM(config)\n",
    "\n",
    "# 모델 파라미터 수 확인\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1000**3:.2f}B parameters\")\n",
    "print(f\"Model size: {model_size/1000**2:.2f}M parameters\")\n",
    "print(f\"Model size: {model_size/1000:.1f}K parameters\")\n",
    "\n",
    "model.save_pretrained(\"./tiny-random\")\n",
    "tokenizer.save_pretrained(\"./tiny-random\")\n",
    "\n",
    "model.push_to_hub(\"minpeter/tiny-ko-random\", private=True)\n",
    "tokenizer.push_to_hub(\"minpeter/tiny-ko-random\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "985991f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n",
      "Content, content.\n",
      "\n",
      "MENENIUS:\n",
      "O sir, you are not right: have you not known\n",
      "The worthiest men have done't?\n",
      "\n",
      "CORIOLANUS:\n",
      "What must I say?\n",
      "'I Pray, sir'--Plague upon't! I cannot bring\n",
      "My tongue to such a pace:--'Look, sir, my wounds!\n",
      "I got them in my country's service, when\n",
      "Some certain of your brethren roar'd and ran\n",
      "From the noise of our own drums.'\n",
      "\n",
      "MENENIUS:\n",
      "O me, the gods!\n",
      "You must not speak of that: you must desire them\n",
      "To think upon you.\n",
      "\n",
      "CORIOLANUS:\n",
      "Think upon me! hang 'em!\n",
      "I would they would forget me, like the virtues\n",
      "Which our divines lose by 'em.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=6\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"./tiny-random\"\n",
    "model_id = \"/data/minpeter/github.com/minpeter/tiny-ko/model/tiny-ko-124m-base\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "All:\n",
    "Content, content.\n",
    "\n",
    "MENENIUS:\n",
    "O sir, you are not right: have you not known\n",
    "The worthiest men have done't?\n",
    "\n",
    "CORIOLANUS:\n",
    "\"\"\".lstrip()\n",
    "\n",
    "sampling_output = pipeline(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_new_tokens=2000\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "print(sampling_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
