import unicodedata
from transformers import AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm

def evaluate_tokenizer(tokenizer_path: str, dataset_name: str, dataset_split: str, num_samples: int = 1000):
    """
    í•™ìŠµëœ í† í¬ë‚˜ì´ì €ì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì /ì •ì„±ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” í•¨ìˆ˜

    Args:
        tokenizer_path (str): í‰ê°€í•  í† í¬ë‚˜ì´ì €ì˜ ê²½ë¡œ
        dataset_name (str): í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„
        dataset_split (str): í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ ìŠ¤í”Œë¦¿ (ì˜ˆ: 'train')
        num_samples (int): í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ ìƒ˜í”Œ ìˆ˜
    """
    print(f"'{tokenizer_path}' í† í¬ë‚˜ì´ì € ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("=" * 50)

    # 1. í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹ ë¡œë“œ
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        # streaming=True ì˜µì…˜ìœ¼ë¡œ ì „ì²´ ë‹¤ìš´ë¡œë“œ ì—†ì´ ì¼ë¶€ë§Œ ë¡œë“œ
        dataset = load_dataset(dataset_name, split=dataset_split, streaming=True)
        # í‰ê°€ë¥¼ ìœ„í•´ ì¼ë¶€ ìƒ˜í”Œë§Œ ê°€ì ¸ì˜¤ê¸°
        samples = list(tqdm(dataset.take(num_samples), total=num_samples, desc="ë°ì´í„°ì…‹ ìƒ˜í”Œ ë¡œë”© ì¤‘"))
        texts = [s['text'] for s in samples if s['text']]
    except Exception as e:
        print(f"í† í¬ë‚˜ì´ì € ë˜ëŠ” ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    # --- ì •ëŸ‰ í‰ê°€ ---
    print("\n[1. ì •ëŸ‰ í‰ê°€]")
    
    # 1-1. ì–´íœ˜ ì§‘í•© í¬ê¸°
    vocab_size = tokenizer.vocab_size
    print(f"  - ì–´íœ˜ ì§‘í•© í¬ê¸° (Vocabulary Size): {vocab_size}")

    # 1-2. ì••ì¶•ë¥  ë° í† í° ë¶„ì„
    total_chars = 0
    total_tokens = 0
    for text in tqdm(texts, desc="ì••ì¶•ë¥  ê³„ì‚° ì¤‘"):
        total_chars += len(text)
        total_tokens += len(tokenizer.encode(text))
    
    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0
    chars_per_token = total_tokens / total_chars if total_chars > 0 else 0
    
    print(f"  - í…ìŠ¤íŠ¸ ì••ì¶•ë¥  (Compression Ratio): {compression_ratio:.2f} (ìºë¦­í„° ìˆ˜ / í† í° ìˆ˜)")
    print(f"  - í† í° ë‹¹ í‰ê·  ê¸€ì ìˆ˜: {1/chars_per_token:.2f}")

    # 1-3. ë‹¨ì–´ ë‹¹ í‰ê·  Subword ê°œìˆ˜ (OOV ê°„ì ‘ í‰ê°€)
    total_words = 0
    total_subwords = 0
    for text in tqdm(texts, desc="Subword ë¶„ì„ ì¤‘"):
        words = text.split() # ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬
        if not words:
            continue
        total_words += len(words)
        total_subwords += len(tokenizer.tokenize(text)) # tokenize()ëŠ” subword ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
    avg_subwords_per_word = total_subwords / total_words if total_words > 0 else 0
    print(f"  - ë‹¨ì–´ ë‹¹ í‰ê·  Subword ê°œìˆ˜: {avg_subwords_per_word:.2f}")
    print("    (ìˆ˜ì¹˜ê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¨ì–´ê°€ í†µì§¸ë¡œ ì–´íœ˜ì§‘ì— ìˆì„ í™•ë¥ ì´ ë†’ìŒ)")


    # --- ì •ì„± í‰ê°€ ---
    print("\n[2. ì •ì„± í‰ê°€ (ìƒ˜í”Œ ë‹¨ì–´ ë¶„ì ˆ í…ŒìŠ¤íŠ¸)]")
    
    sample_words = [
        "í† í¬ë‚˜ì´ì €", "LLM", "ìì—°ì–´ì²˜ë¦¬", "ì–´í…ì…˜", "íŠ¸ëœìŠ¤í¬ë¨¸",
        "ëŒ€í•œë¯¼êµ­", "ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸", "ë”¥ëŸ¬ë‹", "ì¸ê³µì§€ëŠ¥",
        "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤", "ì±—ì§€í”¼í‹°"
    ]
    
    for word in sample_words:
        tokens = tokenizer.tokenize(word)
        print(f"  - '{word}': {tokens}")


    # --- ê°€ì—­ì„±(Reversibility) í…ŒìŠ¤íŠ¸ ---
    print("\n[3. ê°€ì—­ì„±(Reversibility) í…ŒìŠ¤íŠ¸]")
    print("  (ì¸ì½”ë”© -> ë””ì½”ë”© -> ì¬ì¸ì½”ë”© í›„ í† í° ID ì¼ì¹˜ ì—¬ë¶€ë¡œ í™•ì¸)")

    test_sentence = "ì•ˆë…•í•˜ì„¸ìš”! 2024ë…„ì—ë„ LLM ë§Œë“¤ê¸°ëŠ” ì¬ë°Œë„¤ìš”. ğŸ˜‚"

    # 1. ì›ë³¸ ë¬¸ì¥ì„ í† í° IDë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
    #    ìˆœìˆ˜í•œ í† í°ì˜ ê°€ì—­ì„±ì„ ë³´ê¸° ìœ„í•´ íŠ¹ìˆ˜ í† í°(BOS, EOS)ì€ ì œì™¸í•©ë‹ˆë‹¤.
    try:
        original_ids = tokenizer.encode(test_sentence, add_special_tokens=False)

        # 2. ì¸ì½”ë”©ëœ IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤.
        decoded_text = tokenizer.decode(original_ids)

        # 3. ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ í† í° IDë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
        re_encoded_ids = tokenizer.encode(decoded_text, add_special_tokens=False)

        print(f"  - ì›ë³¸ ë¬¸ì¥: {test_sentence}")
        # ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸ëŠ” ì •ê·œí™”(ì˜ˆ: ì†Œë¬¸ìí™”)ê°€ ì ìš©ëœ ìƒíƒœì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        print(f"  - ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: {decoded_text}")
        print(f"  - ì›ë³¸ -> ì¸ì½”ë”© ID: {original_ids}")
        print(f"  - ë””ì½”ë”© -> ì¬ì¸ì½”ë”© ID: {re_encoded_ids}")

        # 4. ë‘ ID ë¦¬ìŠ¤íŠ¸ê°€ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        if original_ids == re_encoded_ids:
            print("  - ê²°ê³¼: âœ… ì™„ë²½í•˜ê²Œ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤ (Round-trip consistency í†µê³¼).")
        else:
            print("  - ê²°ê³¼: âŒ ë³µì› ì‹¤íŒ¨! IDê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"  - ê°€ì—­ì„± í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    print("\n" + "=" * 50)
    print("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    # --- ì„¤ì • ---
    # ì´ì „ì— í•™ìŠµì‹œí‚¨ í† í¬ë‚˜ì´ì €ê°€ ì €ì¥ëœ ê²½ë¡œ
    TOKENIZER_PATH = "/data/minpeter/github.com/minpeter/mirco-ko-llama/tknz/my_llm_tokenizer_for_hf"
    # í† í¬ë‚˜ì´ì € í•™ìŠµì— ì‚¬ìš©í–ˆë˜ ë°ì´í„°ì…‹ (ë˜ëŠ” ìœ ì‚¬í•œ ì„±ê²©ì˜ í…ŒìŠ¤íŠ¸ì…‹)
    DATASET_NAME = "minpeter/pretrain-korean-dedup"
    DATASET_SPLIT = "train"
    # í‰ê°€ì— ì‚¬ìš©í•  ìƒ˜í”Œ ê°œìˆ˜ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    NUM_SAMPLES = 2000

    evaluate_tokenizer(
        tokenizer_path=TOKENIZER_PATH,
        dataset_name=DATASET_NAME,
        dataset_split=DATASET_SPLIT,
        num_samples=NUM_SAMPLES
    )