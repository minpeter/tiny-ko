import unicodedata
from transformers import AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
from typing import List

# HELPER FUNCTION TO DECODE BYTE-LEVEL TOKENS
def decode_byte_tokens(tokens: List[str], tokenizer) -> List[str]:
    """
    Byte-Level BPE í† í¬ë‚˜ì´ì €ì˜ ì•Œì•„ë³¼ ìˆ˜ ì—†ëŠ” í† í° ë¦¬ìŠ¤íŠ¸ë¥¼
    ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    readable_tokens = []
    for token in tokens:
        # ê° í† í°ì„ ë°”ì´íŠ¸ë¡œ ë³€í™˜ í›„ UTF-8ë¡œ ë””ì½”ë”©
        # ì—ëŸ¬ ë°œìƒ ì‹œ ëŒ€ì²´ ë¬¸ìë¡œ í‘œì‹œí•˜ì—¬ í”„ë¡œê·¸ë¨ì´ ë©ˆì¶”ì§€ ì•Šë„ë¡ í•¨
        byte_representation = token.encode('latin-1') # ì‹¬ë³¼ì„ ë°”ì´íŠ¸ë¡œ
        try:
            readable_token = byte_representation.decode('utf-8')
        except UnicodeDecodeError:
            # ê°œë³„ ë°”ì´íŠ¸ í† í°ì´ ì™„ì „í•œ UTF-8 ë¬¸ìë¥¼ í˜•ì„±í•˜ì§€ ëª»í•  ë•Œ ë°œìƒ
            readable_token = repr(byte_representation)
        readable_tokens.append(readable_token)
    return readable_tokens

def evaluate_tokenizer(tokenizer_path: str, dataset_name: str, dataset_split: str, num_samples: int = 1000):
    """
    í•™ìŠµëœ í† í¬ë‚˜ì´ì €ì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì /ì •ì„±ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    """
    print(f"'{tokenizer_path}' í† í¬ë‚˜ì´ì € ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("=" * 50)

    # 1. í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹ ë¡œë“œ
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        dataset = load_dataset(dataset_name, split=dataset_split, streaming=True)
        samples = list(tqdm(dataset.take(num_samples), total=num_samples, desc="ë°ì´í„°ì…‹ ìƒ˜í”Œ ë¡œë”© ì¤‘"))
        texts = [s['text'] for s in samples if s['text']]
    except Exception as e:
        print(f"í† í¬ë‚˜ì´ì € ë˜ëŠ” ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    # --- ì •ëŸ‰ í‰ê°€ ---
    # (ì´ì „ê³¼ ë™ì¼í•˜ì—¬ ìƒëµ, í•„ìš” ì‹œ ìœ„ì˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”)
    print("\n[1. ì •ëŸ‰ í‰ê°€]")
    vocab_size = tokenizer.vocab_size
    print(f"  - ì–´íœ˜ ì§‘í•© í¬ê¸° (Vocabulary Size): {vocab_size}")
    total_chars = sum(len(text) for text in texts)
    total_tokens = sum(len(tokenizer.encode(text)) for text in tqdm(texts, desc="ì••ì¶•ë¥  ê³„ì‚° ì¤‘"))
    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0
    print(f"  - í…ìŠ¤íŠ¸ ì••ì¶•ë¥  (Compression Ratio): {compression_ratio:.2f}")
    total_words = sum(len(text.split()) for text in texts)
    total_subwords = sum(len(tokenizer.tokenize(text)) for text in tqdm(texts, desc="Subword ë¶„ì„ ì¤‘"))
    avg_subwords_per_word = total_subwords / total_words if total_words > 0 else 0
    print(f"  - ë‹¨ì–´ ë‹¹ í‰ê·  Subword ê°œìˆ˜: {avg_subwords_per_word:.2f}")


    # --- ì •ì„± í‰ê°€ ---
    print("\n[2. ì •ì„± í‰ê°€ (ìƒ˜í”Œ ë‹¨ì–´ ë¶„ì ˆ í…ŒìŠ¤íŠ¸)]")
    
    sample_words = [
        "í† í¬ë‚˜ì´ì €", "LLM", "ìì—°ì–´ì²˜ë¦¬", "ì–´í…ì…˜", "íŠ¸ëœìŠ¤í¬ë¨¸",
        "ëŒ€í•œë¯¼êµ­", "ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸", "ë”¥ëŸ¬ë‹", "ì¸ê³µì§€ëŠ¥",
        "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤", "ì±—ì§€í”¼í‹°"
    ]
    
    for word in sample_words:
        tokens = tokenizer.tokenize(word)
        # ğŸ”„ [ìˆ˜ì •] ì•Œì•„ë³¼ ìˆ˜ ì—†ëŠ” ë°”ì´íŠ¸ í† í°ì„ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜
        # tokenizer.convert_tokens_to_string()ì€ ì „ì²´ë¥¼ í•©ì³ë²„ë¦¬ë¯€ë¡œ, ê° í† í°ì„ ê°œë³„ì ìœ¼ë¡œ ë””ì½”ë”©
        
        # ê° í† í°ì„ ê°œë³„ì ìœ¼ë¡œ ë””ì½”ë”©í•˜ì—¬ ëˆˆìœ¼ë¡œ í™•ì¸
        decoded_tokens = []
        for token in tokens:
            # í† í° í•˜ë‚˜ë¥¼ ë””ì½”ë”©í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            decoded_tokens.append(tokenizer.decode([tokenizer.convert_tokens_to_ids(token)]))
            
        # ì›ë³¸ í† í°ê³¼ í•¨ê»˜ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
        original_tokens_str = ' '.join(tokens)
        decoded_tokens_str = ' | '.join(decoded_tokens)
        print(f"  - '{word}': {decoded_tokens}")


    # --- ê°€ì—­ì„±(Reversibility) í…ŒìŠ¤íŠ¸ ---
    # (ì´ì „ê³¼ ë™ì¼í•˜ì—¬ ìƒëµ, í•„ìš” ì‹œ ìœ„ì˜ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”)
    print("\n[3. ê°€ì—­ì„±(Reversibility) í…ŒìŠ¤íŠ¸]")
    test_sentence = "ì•ˆë…•í•˜ì„¸ìš”! 2024ë…„ì—ë„ LLM ë§Œë“¤ê¸°ëŠ” ì¬ë°Œë„¤ìš”. ğŸ˜‚"
    original_ids = tokenizer.encode(test_sentence, add_special_tokens=False)
    decoded_text = tokenizer.decode(original_ids)
    re_encoded_ids = tokenizer.encode(decoded_text, add_special_tokens=False)
    print(f"  - ì›ë³¸ ë¬¸ì¥: {test_sentence}")
    print(f"  - ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: {decoded_text}")
    if original_ids == re_encoded_ids:
        print("  - ê²°ê³¼: âœ… ì™„ë²½í•˜ê²Œ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤ (Round-trip consistency í†µê³¼).")
    else:
        print("  - ê²°ê³¼: âŒ ë³µì› ì‹¤íŒ¨! IDê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    print("\n" + "=" * 50)
    print("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    # --- ì„¤ì • ---
    TOKENIZER_PATH = "/data/minpeter/github.com/minpeter/mirco-ko-llama/tknz/my_llm_tokenizer_for_hf"
    DATASET_NAME = "minpeter/pretrain-korean-dedup"
    DATASET_SPLIT = "train"
    NUM_SAMPLES = 2000

    evaluate_tokenizer(
        tokenizer_path=TOKENIZER_PATH,
        dataset_name=DATASET_NAME,
        dataset_split=DATASET_SPLIT,
        num_samples=NUM_SAMPLES
    )