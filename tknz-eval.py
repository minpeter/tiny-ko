# -*- coding: utf-8 -*-
"""
Hugging Face í† í¬ë‚˜ì´ì € í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì§€ì •ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì—¬ í•œêµ­ì–´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ì„
ì •ëŸ‰ì  ë° ì •ì„±ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

í‰ê°€ ì§€í‘œ:
1.  **ì •ëŸ‰ í‰ê°€:**
    -   ì–´íœ˜ ì§‘í•© í¬ê¸° (Vocabulary Size)
    -   í…ìŠ¤íŠ¸ ì••ì¶•ë¥  (Compression Ratio)
    -   ë‹¨ì–´ ë‹¹ í‰ê·  Subword ê°œìˆ˜
2.  **ì •ì„± í‰ê°€:**
    -   ìƒ˜í”Œ ë‹¨ì–´ ë¶„ì ˆ ë°©ì‹ í™•ì¸
3.  **ê°€ì—­ì„±(Reversibility) í…ŒìŠ¤íŠ¸:**
    -   ì¸ì½”ë”©-ë””ì½”ë”© í›„ ì›ë³¸ ì •ë³´ ë³´ì¡´ ì—¬ë¶€ í™•ì¸
"""

from transformers import AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm

def evaluate_tokenizer(tokenizer_path: str, dataset_name: str, dataset_split: str, num_samples: int = 1000):
    """
    í•™ìŠµëœ í† í¬ë‚˜ì´ì €ì˜ ì„±ëŠ¥ì„ ì •ëŸ‰ì /ì •ì„±ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        tokenizer_path (str): í‰ê°€í•  í† í¬ë‚˜ì´ì €ì˜ ê²½ë¡œ (ë¡œì»¬ ë””ë ‰í† ë¦¬ ë˜ëŠ” Hugging Face Hub ì´ë¦„).
        dataset_name (str): í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì˜ Hugging Face Hub ì´ë¦„.
        dataset_split (str): ì‚¬ìš©í•  ë°ì´í„°ì…‹ì˜ ë¶„í•  (ì˜ˆ: 'train', 'validation').
        num_samples (int, optional): í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ ìƒ˜í”Œì˜ ìˆ˜. Defaults to 1000.
    """
    print(f"'{tokenizer_path}' í† í¬ë‚˜ì´ì € ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("=" * 50)

    # 1. í† í¬ë‚˜ì´ì € ë° ë°ì´í„°ì…‹ ë¡œë“œ
    # streaming=True ì˜µì…˜ì€ ì „ì²´ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šê³ , í•„ìš”í•œ ë§Œí¼ë§Œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        dataset = load_dataset(dataset_name, split=dataset_split, streaming=True)
        # take()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì •ëœ num_samples ë§Œí¼ì˜ ë°ì´í„°ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        samples = list(tqdm(dataset.take(num_samples), total=num_samples, desc="ë°ì´í„°ì…‹ ìƒ˜í”Œ ë¡œë”© ì¤‘"))
        # ë°ì´í„°ì…‹ì—ì„œ 'text' í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤. í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸í•©ë‹ˆë‹¤.
        texts = [s['text'] for s in samples if s['text']]
    except Exception as e:
        print(f"í† í¬ë‚˜ì´ì € ë˜ëŠ” ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    # --- ì •ëŸ‰ í‰ê°€ ---
    # í† í¬ë‚˜ì´ì €ì˜ ì¼ë°˜ì ì¸ ì„±ëŠ¥ì„ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    print("\n[1. ì •ëŸ‰ í‰ê°€]")

    # 1-1. ì–´íœ˜ ì§‘í•© í¬ê¸° (Vocabulary Size)
    # í† í¬ë‚˜ì´ì €ê°€ ì•Œê³  ìˆëŠ” ê³ ìœ  í† í°ì˜ ì´ ê°œìˆ˜ì…ë‹ˆë‹¤.
    vocab_size = tokenizer.vocab_size
    print(f"  - ì–´íœ˜ ì§‘í•© í¬ê¸° (Vocabulary Size): {vocab_size}")

    # 1-2. í…ìŠ¤íŠ¸ ì••ì¶•ë¥  (Compression Ratio)
    # ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ë¬¸ì ìˆ˜ ëŒ€ë¹„ í† í° ìˆ˜ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.
    # ì´ ê°’ì´ í´ìˆ˜ë¡ í•˜ë‚˜ì˜ í† í°ì´ ë” ë§ì€ ì •ë³´ë¥¼ í‘œí˜„í•˜ë¯€ë¡œ ì••ì¶• íš¨ìœ¨ì´ ì¢‹ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    total_chars = sum(len(text) for text in texts)
    # ê° í…ìŠ¤íŠ¸ë¥¼ í† í° ID ë¦¬ìŠ¤íŠ¸ë¡œ ì¸ì½”ë”©í•˜ê³ , ê·¸ ê¸¸ì´ë¥¼ ëª¨ë‘ ë”í•©ë‹ˆë‹¤.
    total_tokens = sum(len(tokenizer.encode(text)) for text in tqdm(texts, desc="ì••ì¶•ë¥  ê³„ì‚° ì¤‘"))
    compression_ratio = total_chars / total_tokens if total_tokens > 0 else 0
    print(f"  - í…ìŠ¤íŠ¸ ì••ì¶•ë¥  (Compression Ratio): {compression_ratio:.2f}")

    # 1-3. ë‹¨ì–´ ë‹¹ í‰ê·  Subword ê°œìˆ˜
    # í•˜ë‚˜ì˜ ë‹¨ì–´(ê³µë°± ê¸°ì¤€)ê°€ í‰ê·  ëª‡ ê°œì˜ í† í°ìœ¼ë¡œ ìª¼ê°œì§€ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    # í•œêµ­ì–´ì™€ ê°™ì´ êµì°©ì–´ì˜ ê²½ìš°, ì´ ê°’ì´ ë„ˆë¬´ í¬ë©´ ì˜ë¯¸ ë‹¨ìœ„ê°€ ì§€ë‚˜ì¹˜ê²Œ ë¶„ì ˆëœë‹¤ëŠ” ì˜ë¯¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¨ì–´ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§•ì´ ì˜ ëœë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    total_words = sum(len(text.split()) for text in texts)
    total_subwords = sum(len(tokenizer.tokenize(text)) for text in tqdm(texts, desc="Subword ë¶„ì„ ì¤‘"))
    avg_subwords_per_word = total_subwords / total_words if total_words > 0 else 0
    print(f"  - ë‹¨ì–´ ë‹¹ í‰ê·  Subword ê°œìˆ˜: {avg_subwords_per_word:.2f}")


    # --- ì •ì„± í‰ê°€ ---
    # ì‹¤ì œ ë‹¨ì–´ë“¤ì´ ì–´ë–»ê²Œ ë¶„ì ˆë˜ëŠ”ì§€ ì§ì ‘ ëˆˆìœ¼ë¡œ í™•ì¸í•˜ì—¬ í† í¬ë‚˜ì´ì €ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
    print("\n[2. ì •ì„± í‰ê°€ (ìƒ˜í”Œ ë‹¨ì–´ ë¶„ì ˆ í…ŒìŠ¤íŠ¸)]")

    # í‰ê°€í•  ë‹¨ì–´ ëª©ë¡: ì¼ë°˜ ëª…ì‚¬, ì „ë¬¸ ìš©ì–´(LLM), ì‹ ì¡°ì–´, ë³µí•© ëª…ì‚¬ ë“±
    sample_words = [
        "í† í¬ë‚˜ì´ì €", "LLM", "ìì—°ì–´ì²˜ë¦¬", "ì–´í…ì…˜", "íŠ¸ëœìŠ¤í¬ë¨¸",
        "ëŒ€í•œë¯¼êµ­", "ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸", "ë”¥ëŸ¬ë‹", "ì¸ê³µì§€ëŠ¥",
        "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤", "ì±—ì§€í”¼í‹°", "ì¸ìŠ¤íƒ€ê·¸ë¨", "ìœ íŠœë¸Œ",
        "Hello, World!", "Python", "TensorFlow", "PyTorch",
        "ì…ë‹ˆë‹¤", ". í•™ìƒ ì—¬ëŸ¬ë¶„.",
    ]

    for word in sample_words:
        # ì£¼ì–´ì§„ ë‹¨ì–´ë¥¼ í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•´ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ì ˆí•©ë‹ˆë‹¤.
        tokens = tokenizer.tokenize(word)

        # ê° í† í°ì„ ê°œë³„ì ìœ¼ë¡œ ë””ì½”ë”©í•˜ì—¬ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        # `tokenizer.convert_tokens_to_string(tokens)`ëŠ” ëª¨ë“  í† í°ì„ í•©ì³ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë§Œë“¤ê¸° ë•Œë¬¸ì—,
        # ê°œë³„ í† í°ì˜ ë¶„ì ˆ ìƒíƒœë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ê° í† í°ì„ ë”°ë¡œ ë””ì½”ë”©í•´ì•¼ í•©ë‹ˆë‹¤.
        decoded_tokens = []
        for token in tokens:
            # í† í° í•˜ë‚˜ë¥¼ IDë¡œ ë³€í™˜í•œ í›„, ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ ì›ë˜ ë¬¸ìì—´ ì¡°ê°ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
            # ì´ ê³¼ì •ì„ í†µí•´ 'Ä ' ê°™ì€ íŠ¹ìˆ˜ ê¸°í˜¸ê°€ ê³µë°±ìœ¼ë¡œ ë³€í™˜ë˜ëŠ” ë“± ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³´ì…ë‹ˆë‹¤.
            token_id = tokenizer.convert_tokens_to_ids(token)
            decoded_token = tokenizer.decode([token_id])
            decoded_tokens.append(decoded_token)

        # ë¶„ì ˆëœ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
        print(f"  - '{word}': {decoded_tokens}")


    # --- ê°€ì—­ì„±(Reversibility) í…ŒìŠ¤íŠ¸ ---
    # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”(ì¸ì½”ë”©)í–ˆë‹¤ê°€ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³µì›(ë””ì½”ë”©)í–ˆì„ ë•Œ, ì›ë³¸ ì •ë³´ê°€ ì†ì‹¤ë˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    # "Round-trip consistency"ë¼ê³ ë„ ë¶€ë¦…ë‹ˆë‹¤.
    print("\n[3. ê°€ì—­ì„±(Reversibility) í…ŒìŠ¤íŠ¸]")
    test_sentences = [
        "ì´ê²ƒì€ í† í¬ë‚˜ì´ì € ê°€ì—­ì„± í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
        "ì±—ì§€í”¼í‹°ì™€ í•¨ê»˜í•˜ëŠ” ìì—°ì–´ì²˜ë¦¬ ì‹¤ìŠµ.",
        "Pythonì€ ì •ë§ ê°•ë ¥í•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
        "ë°ì´í„° ê³¼í•™ê³¼ ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ëŠ” ë°ë‹¤!",
        "ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤",
        "í•œê¸€ê³¼ ì˜ì–´ê°€ ì„ì¸ ë¬¸ì¥ì…ë‹ˆë‹¤. Hello, world!",

        # ì˜ì–´ ë¬¸ì¥
        "This is a test sentence for the tokenizer.",
        "Let's see how well it handles different languages.",
        "The quick brown fox jumps over the lazy dog.",

        # ì½”ë“œ
        "def hello_world():\n    print('Hello, World!')",
        "import numpy as np\n\n# NumPy ë°°ì—´ ìƒì„±\narr = np.array([1, 2, 3, 4, 5])",
        "class Tokenizer:\n    def __init__(self):\n        pass\n\n    def tokenize(self, text):\n        return text.split()",
        '{"name": "ChatGPT", "version": "4.0", "features": ["NLP", "AI", "ML"]}',
        "<tool_call>\n{\"name\": \"similar_string\", \"arguments\": {\"str1\": \"kitten\", \"str2\": \"sitting\", \"error_limit\": 3}}\n</tool_call>\n<tool_call>\n{\"name\": \"similar_string\", \"arguments\": {\"str1\": \"hello\", \"str2\": \"world\", \"error_limit\": 1}}\n</tool_call>",
        "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> {\"type\": \"function\", \"function\": {\"name\": \"distance_to_miles\", \"description\": \"distance_to_miles(meters: float) - Converts a distance measured in meters to miles.\n\n Args:\n meters(float): The distance in meters to be converted.\", \"parameters\": {\"additionalProperties\": false, \"properties\": {\"meters\": {\"description\": \"The distance in meters to be converted.\", \"type\": \"number\"}}, \"required\": [\"meters\"], \"type\": \"object\"}}\n{\"type\": \"function\", \"function\": {\"name\": \"similar_string\", \"description\": \"similar_string(str1: str, str2: str, error_limit: int) - Checks if two strings are similar within the specified error limit.\n\n Args:\n str1(str): The first string to compare. str2(str): The second string to compare. error_limit(int): The maximum allowed difference between the strings.\", \"parameters\": {\"additionalProperties\": false, \"properties\": {\"error_limit\": {\"description\": \"The maximum allowed difference between the strings.\", \"type\": \"integer\"}, \"str1\": {\"description\": \"The first string to compare.\", \"type\": \"string\"}, \"str2\": {\"description\": \"The second string to compare.\", \"type\": \"string\"}}, \"required\": [\"str1\", \"str2\", \"error_limit\"], \"type\": \"object\"}}\n{\"type\": \"function\", \"function\": {\"name\": \"format_list_of_objects\", \"description\": \"format_list_of_objects(objects: list) - Converts a list of objects into a formatted string.\n\nEach object is converted to a string and separated by commas.\nIf the object is a string, it is surrounded by single quotes.\nIf the object is a number, it is not surrounded by quotes.\n\n Args:\n objects(list): A list of objects to be formatted.\", \"parameters\": {\"additionalProperties\": false, \"properties\": {\"objects\": {\"description\": \"A list of objects to be formatted.\", \"items\": {\"type\": [\"integer\", \"number\", \"string\"]}, \"type\": \"array\"}}, \"required\": [\"objects\"], \"type\": \"object\"}} </tools>Use the following pydantic model json schema for each tool call you will make: {\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"type\": \"object\"}}, \"required\": [\"name\", \"arguments\"], \"title\": \"FunctionCall\", \"type\": \"object\"}}\nFor each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-dict>}\n</tool_call>",
        "https://huggingface.co/dataset/minpeter/tiny-ko-corpus",

        # mixed emojis and special characters
        "ì´ëª¨ì§€ ğŸ˜Šì™€ íŠ¹ìˆ˜ ë¬¸ì #, @, !, $ê°€ ì„ì¸ ë¬¸ì¥ì…ë‹ˆë‹¤.",
        "ì´ ë¬¸ì¥ì€ ë‹¤ì–‘í•œ íŠ¹ìˆ˜ ë¬¸ìì™€ ì´ëª¨ì§€ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤! ğŸ˜Š #Python @AI $DataScience",
    ]

    for idx, test_sentence in enumerate(test_sentences, 1):
        # 1. ì›ë³¸ ë¬¸ì¥ì„ í† í° ID ë¦¬ìŠ¤íŠ¸ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤. (íŠ¹ìˆ˜ í† í° ì œì™¸)
        original_ids = tokenizer.encode(test_sentence, add_special_tokens=False)
        # 2. ì¸ì½”ë”©ëœ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤.
        decoded_text = tokenizer.decode(original_ids)
        # 3. ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.
        re_encoded_ids = tokenizer.encode(decoded_text, add_special_tokens=False)

        print(f"\n  [{idx}] ì›ë³¸ ë¬¸ì¥: {test_sentence}")
        print(f"      ë””ì½”ë”©ëœ í…ìŠ¤íŠ¸: {decoded_text}")

        # 4. ì›ë³¸ IDì™€ ì¬ì¸ì½”ë”©ëœ IDê°€ ì™„ì „íˆ ì¼ì¹˜í•˜ëŠ”ì§€ ë¹„êµí•©ë‹ˆë‹¤.
        if original_ids == re_encoded_ids:
            print("      ê²°ê³¼: âœ… ì™„ë²½í•˜ê²Œ ë³µì›ë˜ì—ˆìŠµë‹ˆë‹¤ (Round-trip consistency í†µê³¼).")
        else:
            print("      ê²°ê³¼: âŒ ë³µì› ì‹¤íŒ¨! IDê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"        - ì›ë³¸ ID: {original_ids}")
            print(f"        - ì¬ì¸ì½”ë”© ID: {re_encoded_ids}")


    print("\n" + "=" * 50)
    print("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    # --- ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì„¤ì • ---
    # ì‚¬ìš©ìê°€ ìì‹ ì˜ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•´ì•¼ í•  ë¶€ë¶„ì…ë‹ˆë‹¤.

    # í‰ê°€í•  í† í¬ë‚˜ì´ì €ì˜ ê²½ë¡œ (ë¡œì»¬ ê²½ë¡œ ë˜ëŠ” Hugging Face Hub ê²½ë¡œ)
    TOKENIZER_PATH = "./tknz/tiny-ko-tokenizer"
    # í‰ê°€ì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì˜ Hugging Face Hub ê²½ë¡œ
    DATASET_NAME = "minpeter/tiny-ko-corpus"
    # ì‚¬ìš©í•  ë°ì´í„°ì…‹ì˜ ì¢…ë¥˜ (ì˜ˆ: 'train', 'validation', 'test')
    DATASET_SPLIT = "train"
    # ì •ëŸ‰ í‰ê°€ì— ì‚¬ìš©í•  ìƒ˜í”Œì˜ ê°œìˆ˜
    NUM_SAMPLES = 2000

    # ì„¤ì •ëœ ê°’ìœ¼ë¡œ í† í¬ë‚˜ì´ì € í‰ê°€ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    evaluate_tokenizer(
        tokenizer_path=TOKENIZER_PATH,
        dataset_name=DATASET_NAME,
        dataset_split=DATASET_SPLIT,
        num_samples=NUM_SAMPLES
    )