import transformers
import torch
import argparse
import sys
from threading import Thread
from transformers import TextIteratorStreamer, AutoTokenizer, GenerationConfig

def main():
    """
    Hugging Face ëª¨ë¸ê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ” CLI ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
    TextIteratorStreamerë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.
    """
    parser = argparse.ArgumentParser(description="Interactive CLI chat with a Hugging Face model.")
    parser.add_argument("--model", type=str, required=True, help="Hugging Face model ID (e.g., minpeter/tiny-ko-124m-sft-muon)")
    args = parser.parse_args()

    model_id = args.model

    print(f"âš“ï¸ Loading model: {model_id}...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        
        pipeline = transformers.pipeline(
            "text-generation",
            model=model_id,
            tokenizer=tokenizer,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device_map="auto",
        )
        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
        
        print("Model loaded successfully! Type 'quit' or 'exit' to end the chat. ğŸ´â€â˜ ï¸")
    
    except Exception as e:
        print(f"Arr, matey! Couldn't load the model: {e}")
        sys.exit(1)

    messages = []
    
    generation_config = GenerationConfig(
        max_new_tokens=256,
        do_sample=True,
        top_p=0.9,
        temperature=0.7,
    )

    while True:
        try:
            user_input = input("You: ")
            if user_input.lower() in ["quit", "exit"]:
                print("Farewell, matey! Until next time. â›µï¸")
                break

            messages.append({"role": "user", "content": user_input})

            prompt = pipeline.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            # --- ğŸ’¡ ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„ì…ë‹ˆë‹¤! ---
            # í‚¤ì›Œë“œ ì¸ì(kwargs)ëŠ” streamerì™€ generation_configë§Œ í¬í•¨í•˜ë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.
            generation_kwargs = {
                "streamer": streamer,
                "generation_config": generation_config,
            }

            # 'prompt'ëŠ” ìœ„ì¹˜ ì¸ì(args)ë¡œ, ë‚˜ë¨¸ì§€ëŠ” í‚¤ì›Œë“œ ì¸ì(kwargs)ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
            # 'args'ëŠ” íŠœí”Œì´ë‚˜ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•˜ë¯€ë¡œ [prompt]ë¡œ ê°ì‹¸ì¤ë‹ˆë‹¤.
            thread = Thread(target=pipeline, args=[prompt], kwargs=generation_kwargs)
            # --- ìˆ˜ì • ë ---
            
            thread.start()

            print("Pirate: ", end="")
            full_response = ""
            for new_text in streamer:
                sys.stdout.write(new_text)
                sys.stdout.flush()
                full_response += new_text

            print()
            
            thread.join()
            
            messages.append({"role": "assistant", "content": full_response.strip()})

        except KeyboardInterrupt:
            print("\nFarewell, matey! Until next time. â›µï¸")
            break
        except Exception as e:
            # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ì¹œì ˆí•˜ê²Œ ë³´ì—¬ì¤ë‹ˆë‹¤.
            print(f"\nShiver me timbers! An error occurred: {e}")
            if messages and messages[-1]["role"] == "user":
                messages.pop()


if __name__ == "__main__":
    main()