import os
import evaluate
import torch
from transformers import (
    AutoTokenizer,
    LlamaForCausalLM,
    AutoConfig,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
)
from datasets import load_dataset

ds = load_dataset('minpeter/pretrain-korean-dedup', split='train')
ds = ds.train_test_split(test_size=0.001, shuffle=True, seed=5768112)
print(ds)

context_length = 2048
max_cpu_count = int(os.cpu_count() / 3) or 1

tokenizer = AutoTokenizer.from_pretrained("kakaocorp/kanana-nano-2.1b-base")

if tokenizer.eos_token is None:
    raise ValueError("í† í¬ë‚˜ì´ì €ì— EOS í† í°ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ì „ì— EOS í† í°ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
if tokenizer.pad_token is None:
    print("í† í¬ë‚˜ì´ì €ì— PAD í† í°ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šì•„ EOS í† í°ì„ PAD í† í°ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    tokenizer.pad_token = tokenizer.eos_token

print(f"ì‚¬ìš©ë  EOS í† í°: '{tokenizer.eos_token}', ID: {tokenizer.eos_token_id}")
print(f"ì‚¬ìš©ë  BOS í† í°: '{tokenizer.bos_token}', ID: {tokenizer.bos_token_id}")
print(f"ì‚¬ìš©ë  PAD í† í°: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}") # ì´ì œ EOS í† í°ê³¼ ë™ì¼í•˜ê±°ë‚˜, ì›ë˜ PAD í† í° ê°’

def append_eos_to_text(examples):
    processed_texts = [text + tokenizer.eos_token for text in examples['text']]
    examples['text'] = processed_texts
    return examples

print("\nê° ë¬¸ì„œì— EOS í† í° ì¶”ê°€ ì¤‘...")
ds_with_eos = ds.map(
    append_eos_to_text,
    batched=True,
    batch_size=5_000,
    num_proc=max_cpu_count
)
print("EOS ì¶”ê°€ í›„ ë°ì´í„°ì…‹ ìƒ˜í”Œ (text í•„ë“œë§Œ):")
print(ds_with_eos["train"][0]['text'][-100:])

def tokenize(element):
    outputs = tokenizer(
        element["text"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    return outputs

print("\ní† í°í™” ì§„í–‰ ì¤‘...")
tokenized_dataset = ds_with_eos.map(
    tokenize,
    remove_columns=ds_with_eos["train"].column_names,
    batched=True,
    batch_size=5_000,
    num_proc=max_cpu_count,
)
print("í† í°í™”ëœ ë°ì´í„°ì…‹ êµ¬ì¡°:")
print(tokenized_dataset)
print("í† í°í™”ëœ ë°ì´í„°ì…‹ ìƒ˜í”Œ (input_ids):")
for i in range(min(5, len(tokenized_dataset["train"]))):
    last_tokens = tokenized_dataset["train"][i]['input_ids'][-5:]
    print(f"ìƒ˜í”Œ {i}ì˜ ë§ˆì§€ë§‰ 5ê°œ í† í° ID: {last_tokens}, EOS IDì™€ ë¹„êµ: {tokenizer.eos_token_id}")
    if tokenizer.eos_token_id in last_tokens:
        print(f"  ìƒ˜í”Œ {i}ì˜ ë§ˆì§€ë§‰ì— EOS í† í° ID({tokenizer.eos_token_id})ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")


# ğŸš€ ëª¨ë¸ ì´ˆê¸°í™” (vocab_sizeëŠ” í† í¬ë‚˜ì´ì € ê¸¸ì´ì— ë§ì¶¤)
config = AutoConfig.from_pretrained(
    "HuggingFaceTB/SmolLM2-135M",
    vocab_size=len(tokenizer),    # EOS ë˜ëŠ” ë‹¤ë¥¸ í† í° ì¶”ê°€ë¡œ ì¸í•´ tokenizer ê¸¸ì´ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŒ
    max_position_embeddings=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id, # ìœ„ì—ì„œ ì„¤ì •ëœ tokenizer.eos_token_id ì‚¬ìš©
    pad_token_id=tokenizer.pad_token_id, # ìœ„ì—ì„œ ì„¤ì •ëœ tokenizer.pad_token_id ì‚¬ìš© (eos_token_idì™€ ê°™ì„ ìˆ˜ ìˆìŒ)
)

model = LlamaForCausalLM(config)
# ëª¨ë¸ configì—ë„ pad_token_idë¥¼ ëª…ì‹œì ìœ¼ë¡œ ë‹¤ì‹œ í•œë²ˆ ì„¤ì • (config ê°ì²´ì™€ model.configê°€ ë™ì¼ ì°¸ì¡°ê°€ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
model.config.pad_token_id = tokenizer.pad_token_id

model_size = sum(t.numel() for t in model.parameters())
print(f"\nëª¨ë¸ í¬ê¸°: {model_size/1000**3:.1f}B parameters")

def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
        logits = logits[0]
    return torch.argmax(logits, axis=-1)

metric = evaluate.load("accuracy")

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # preds have the same shape as the labels, after the argmax(-1) has been calculated
    # by preprocess_logits_for_metrics but we need to shift the labels
    labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    mask = labels != -100
    labels = labels[mask]
    preds = preds[mask]
    return metric.compute(predictions=preds, references=labels)


data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

hf_model_id = "minpeter/pretrained-tiny-ko"
local_model_path = "model/pretrained-tiny-ko"

tokenizer.save_pretrained(local_model_path)
tokenizer.push_to_hub(hf_model_id)

args = TrainingArguments(
    output_dir=local_model_path,
    push_to_hub=True,
    hub_model_id=hf_model_id,
    hub_strategy="every_save",

    eval_strategy="steps",
    save_strategy="steps",
    eval_steps=1_000,
    save_steps=1_000,

    gradient_accumulation_steps=4,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,

    logging_steps=5,
    num_train_epochs=2,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    bf16=True,
    save_total_limit=5,
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",
    greater_is_better=True,
)

trainer = Trainer(
    model=model,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
    preprocess_logits_for_metrics=preprocess_logits_for_metrics,
)

trainer.train()