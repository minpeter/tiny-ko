import os
import math
import evaluate
import torch
from transformers import (
    AutoTokenizer,
    LlamaForCausalLM,
    AutoConfig,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
)
from datasets import load_dataset, DatasetDict

# ğŸš€ ë°ì´í„° ë¡œë“œ
print("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ('minpeter/pretrain-korean-dedup')...")
full_ds = load_dataset('minpeter/pretrain-korean-dedup', split='train')
# ì‹¤ì œ í•™ìŠµ ì‹œì—ëŠ” test_sizeë¥¼ ì ì ˆíˆ ì¡°ì ˆ (ì˜ˆ: 0.001 ~ 0.01)
ds_split = full_ds.train_test_split(test_size=0.01, shuffle=True, seed=5768112)
ds = DatasetDict({
    'train': ds_split['train'],
    'test': ds_split['test']
})
print("ì›ë³¸ ë°ì´í„°ì…‹ êµ¬ì¡°:")
print(ds)
print(f"í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(ds['train'])}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(ds['test'])}")

max_cpu_core = int(os.cpu_count() / 4) if os.cpu_count() else 1 # cpu_countê°€ Noneì¼ ê²½ìš° ëŒ€ë¹„

# ğŸš€ í† í¬ë‚˜ì´ì € ë¡œë“œ ë° EOS/PAD í† í° ì„¤ì •
context_length = 2048
tokenizer_name = "kakaocorp/kanana-nano-2.1b-base"
print(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ('{tokenizer_name}')...")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

print(f"ì›ë³¸ í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸° (vocab_size): {len(tokenizer)}")

if tokenizer.eos_token is None:
    print("âš ï¸ ê²½ê³ : í† í¬ë‚˜ì´ì €ì— ê¸°ë³¸ EOS í† í°ì´ ì—†ìŠµë‹ˆë‹¤. '</s>'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
    tokenizer.add_special_tokens({'eos_token': '</s>'})

if tokenizer.pad_token is None:
    print("í† í¬ë‚˜ì´ì €ì— PAD í† í°ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šì•„ EOS í† í°ì„ PAD í† í°ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    tokenizer.pad_token = tokenizer.eos_token

print(f"ì‚¬ìš©ë  EOS í† í°: '{tokenizer.eos_token}', ID: {tokenizer.eos_token_id}")
print(f"ì‚¬ìš©ë  BOS í† í°: '{tokenizer.bos_token}', ID: {tokenizer.bos_token_id}")
print(f"ì‚¬ìš©ë  PAD í† í°: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}")
print(f"ìµœì¢… í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸° (vocab_size): {len(tokenizer)}")


# ğŸš€ ê° ë¬¸ì„œ ëì— EOS í† í° ì¶”ê°€
def append_eos_to_text(examples):
    processed_texts = [text + tokenizer.eos_token for text in examples['text']]
    examples['text'] = processed_texts
    return examples

print("\nê° ë¬¸ì„œì— EOS í† í° ì¶”ê°€ ì¤‘...")
ds_with_eos = ds.map(
    append_eos_to_text,
    batched=True,
    batch_size=5_000,
    num_proc=max_cpu_core
)
print("EOS ì¶”ê°€ í›„ ë°ì´í„°ì…‹ ìƒ˜í”Œ (text í•„ë“œë§Œ):")
if len(ds_with_eos["train"]) > 0:
    print(ds_with_eos["train"][0]['text'][-100:])
else:
    print("í•™ìŠµ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆì–´ EOS ì¶”ê°€ ìƒ˜í”Œì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# ğŸš€ ì´ˆê¸° í† í°í™” (í…ìŠ¤íŠ¸ -> í† í° ID ë¦¬ìŠ¤íŠ¸)
def tokenize_for_grouping(element):
    return tokenizer(element["text"], truncation=False, return_attention_mask=False)

print("\nì´ˆê¸° í† í°í™” ì§„í–‰ ì¤‘...")
tokenized_dataset_for_grouping = ds_with_eos.map(
    tokenize_for_grouping,
    batched=True,
    batch_size=5_000,
    num_proc=max_cpu_core,
    remove_columns=ds_with_eos["train"].column_names
)

# ğŸš€ í…ìŠ¤íŠ¸ ê·¸ë£¹í™” (Concatenate and Chunk)
def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])

    if total_length >= context_length:
        total_length = (total_length // context_length) * context_length
    
    result = {
        k: [t[i : i + context_length] for i in range(0, total_length, context_length)]
        for k, t in concatenated_examples.items()
    }
    return result

print("\ní† í°í™”ëœ í…ìŠ¤íŠ¸ ê·¸ë£¹í™” ì¤‘...")
lm_datasets = tokenized_dataset_for_grouping.map(
    group_texts,
    batched=True,
    batch_size=500, # ê·¸ë£¹í™” ì‹œ ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ì¡°ì ˆ)
    num_proc=max_cpu_core,
)
print("ê·¸ë£¹í™”ëœ ë°ì´í„°ì…‹ êµ¬ì¡°:")
print(lm_datasets)
if len(lm_datasets["train"]) > 0:
    print("ê·¸ë£¹í™”ëœ í•™ìŠµ ë°ì´í„°ì…‹ ì²« ìƒ˜í”Œ input_ids ê¸¸ì´:", len(lm_datasets["train"][0]['input_ids']))
    print(f"ê·¸ë£¹í™” í›„ í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(lm_datasets['train'])}")
    print(f"ê·¸ë£¹í™” í›„ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(lm_datasets['test'])}")
else:
    print("ê·¸ë£¹í™”ëœ í•™ìŠµ ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. context_length ë˜ëŠ” ë°ì´í„° ì–‘ì„ í™•ì¸í•˜ì„¸ìš”.")


# ğŸš€ ëª¨ë¸ ì´ˆê¸°í™”
model_config_name = "HuggingFaceTB/SmolLM2-135M"
print(f"ëª¨ë¸ ì„¤ì • ë¡œë“œ ì¤‘ ('{model_config_name}')...")
config = AutoConfig.from_pretrained(
    model_config_name,
    vocab_size=len(tokenizer),
    max_position_embeddings=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

print(f"LlamaForCausalLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ (vocab_size={config.vocab_size}, max_pos_emb={config.max_position_embeddings})...")
model = LlamaForCausalLM(config)

model_size = sum(t.numel() for t in model.parameters())
print(f"\nëª¨ë¸ í¬ê¸°: {model_size/1000**2:.1f}M parameters")


# ğŸš€ í‰ê°€ ì§€í‘œ ì¤€ë¹„
accuracy_metric = evaluate.load("accuracy")

def preprocess_logits_for_metrics(logits, _):
    if isinstance(logits, tuple): # ëª¨ë¸ ì¶œë ¥ì—ì„œ logits ì¶”ì¶œ
        return logits[0]
    return logits

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    
    # Perplexity ê³„ì‚°
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    # DataCollatorForLanguageModelingì€ ë ˆì´ë¸”ì˜ íŒ¨ë”© ë¶€ë¶„ì„ -100ìœ¼ë¡œ ì±„ìš°ë¯€ë¡œ, ignore_indexë¥¼ -100ìœ¼ë¡œ ì„¤ì •
    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    perplexity = math.exp(loss.item())

    # Accuracy ê³„ì‚°
    predictions = torch.argmax(logits, axis=-1)
    labels_for_acc = labels[:, 1:].reshape(-1) # labelsì—ì„œ ì²«ë²ˆì§¸ í† í° ì œì™¸
    preds_for_acc = predictions[:, :-1].reshape(-1) # predictionsì—ì„œ ë§ˆì§€ë§‰ í† í° ì œì™¸
    
    # DataCollatorForLanguageModelingì´ íŒ¨ë”©ì„ -100ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ, í•´ë‹¹ ê°’ì„ ë§ˆìŠ¤í¬ ì²˜ë¦¬
    mask = labels_for_acc != -100

    valid_labels = labels_for_acc[mask]
    valid_preds = preds_for_acc[mask]
    accuracy = accuracy_metric.compute(predictions=valid_preds, references=valid_labels)["accuracy"]
    
    return {
        "perplexity": perplexity,
        "accuracy": accuracy,
        "eval_loss": loss.item() # eval_lossë¥¼ ì¶”ê°€í•˜ì—¬ Trainer ë¡œê·¸ì™€ ì¼ê´€ì„± ìœ ì§€
    }


# ğŸš€ ë°ì´í„° ì½œë ˆì´í„°
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
print(f"ë°ì´í„° ì½œë ˆì´í„°ì˜ pad_token_idëŠ” {tokenizer.pad_token_id}ì´ì§€ë§Œ, ë ˆì´ë¸”ì—ì„œëŠ” -100ìœ¼ë¡œ íŒ¨ë”© ì²˜ë¦¬ë©ë‹ˆë‹¤.")


# ğŸš€ TrainingArguments
output_dir_name = "model/pretrained-tiny-ko-eos"
hub_id = "minpeter/pretrain-tiny-ko-eos" # Hugging Face Hub ID

# í•™ìŠµ ì „ í† í¬ë‚˜ì´ì € ì €ì¥ ë° Hub ì—…ë¡œë“œ
tokenizer.save_pretrained(output_dir_name)
try:
    tokenizer.push_to_hub(hub_id)
    print(f"í† í¬ë‚˜ì´ì €ê°€ '{hub_id}' Hubì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except Exception as e:
    print(f"í† í¬ë‚˜ì´ì € Hub ì—…ë¡œë“œ ì‹¤íŒ¨: {e}. ë¡œì»¬ì—ë§Œ ì €ì¥ë©ë‹ˆë‹¤: '{output_dir_name}'")


args = TrainingArguments(
    output_dir=output_dir_name,

    push_to_hub=True,
    hub_model_id=hub_id,
    hub_strategy="every_save",

    eval_strategy="steps",
    save_strategy="steps",
    eval_steps=1000,
    save_steps=1000,

    gradient_accumulation_steps=4,
    per_device_train_batch_size=8, # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì •
    per_device_eval_batch_size=2,  # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì •

    logging_steps=50, # ë¡œê·¸ ë¹ˆë„ (ê¸°ì¡´ 5ì—ì„œ ëŠ˜ë¦¼, ë„ˆë¬´ ì¦ì€ ë¡œê·¸ ë°©ì§€)
    num_train_epochs=2,
    weight_decay=0.1,
    warmup_steps=200,
    lr_scheduler_type="cosine",
    learning_rate=1e-4,
    
    # BF16/FP16 ì„¤ì •: ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° BF16 ìš°ì„ , ì•„ë‹ˆë©´ FP16
    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8,
    fp16=torch.cuda.is_available() and not (torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8),

    save_total_limit=5,
    load_best_model_at_end=True,
    metric_for_best_model="eval_perplexity", # perplexityê°€ ë‚®ì€ ëª¨ë¸ì„ ìµœì„ ìœ¼ë¡œ ì„ íƒ
    greater_is_better=False,

    # torch_compile=True, # PyTorch 2.0+ ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ì‚¬ìš© ê°€ëŠ¥ (ì²« ìŠ¤í… ëŠë¦´ ìˆ˜ ìˆìŒ)
    report_to="wandb", # "none", "wandb", "tensorboard" ë“±
    # deepspeed="ds_config.json", # DeepSpeed ì‚¬ìš© ì‹œ ì„¤ì • íŒŒì¼ ê²½ë¡œ
)

trainer = Trainer(
    model=model,
    args=args,
    data_collator=data_collator,
    train_dataset=lm_datasets["train"] if lm_datasets and len(lm_datasets["train"]) > 0 else None,
    eval_dataset=lm_datasets["test"] if lm_datasets and len(lm_datasets["test"]) > 0 else None,
    compute_metrics=compute_metrics,
    preprocess_logits_for_metrics=preprocess_logits_for_metrics,
)

print("\ní›ˆë ¨ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ!")

if trainer.train_dataset and trainer.eval_dataset:
    num_devices = max(1, args.world_size) # ë¶„ì‚° í•™ìŠµ ê³ ë ¤
    effective_batch_size = args.per_device_train_batch_size * args.gradient_accumulation_steps * num_devices
    total_train_samples = len(lm_datasets['train'])
    steps_per_epoch = math.ceil(total_train_samples / effective_batch_size)
    total_steps = steps_per_epoch * args.num_train_epochs
    print(f"ì´ í•™ìŠµ ìŠ¤í… ìˆ˜ ì˜ˆìƒ: {total_steps} (ì—í¬í¬ ë‹¹ {steps_per_epoch} ìŠ¤í…)")
    
    print("í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    trainer.train()
    print("í•™ìŠµ ì™„ë£Œ!")

    # í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ Hubì— í‘¸ì‹œ (load_best_model_at_end=True ì´ë¯€ë¡œ ìµœì  ëª¨ë¸)
    try:
        trainer.push_to_hub()
        print("ìµœì¢… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì €ê°€ Hubì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ìµœì¢… ëª¨ë¸ Hub ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

else:
    print("í•™ìŠµ ë˜ëŠ” í‰ê°€ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ë°ì´í„°ì…‹ ë¡œë“œ, í† í°í™”, ê·¸ë£¹í™” ê³¼ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    if lm_datasets:
        print(f"í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(lm_datasets.get('train', []))}")
        print(f"í‰ê°€ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(lm_datasets.get('test', []))}")