import os
import math
import evaluate
import torch
from transformers import (
    AutoTokenizer,
    LlamaForCausalLM,
    AutoConfig,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
)
from datasets import load_dataset, DatasetDict

# ğŸš€ ë°ì´í„° ë¡œë“œ
print("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ('minpeter/pretrain-korean-dedup')... ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
full_ds = load_dataset('minpeter/pretrain-korean-dedup', split='train')
# train_test_splitì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ ë¶„í•  (ì˜ˆ: 99% train, 1% test)
# ì‹¤ì œ í•™ìŠµ ì‹œì—ëŠ” test_sizeë¥¼ ì ì ˆíˆ ì¡°ì ˆí•˜ì„¸ìš” (ì˜ˆ: 0.001 ~ 0.01)
ds_split = full_ds.train_test_split(test_size=0.01, shuffle=True, seed=5768112) # 0.5% for test
ds = DatasetDict({
    'train': ds_split['train'],
    'test': ds_split['test']
})
print("ì›ë³¸ ë°ì´í„°ì…‹ êµ¬ì¡°:")
print(ds)
print(f"í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(ds['train'])}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(ds['test'])}")


# ğŸš€ í† í¬ë‚˜ì´ì € ë¡œë“œ ë° EOS/PAD í† í° í™•ì¸ ë° ì„¤ì •
context_length = 2048 # ì›ë˜ context_lengthë¡œ ë³µì›
tokenizer_name = "kakaocorp/kanana-nano-2.1b-base" # ì›ë˜ í† í¬ë‚˜ì´ì €ë¡œ ë³µì›
print(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ('{tokenizer_name}')...")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
tokenizer.chat_template = None # reset chat template

print(f"ì›ë³¸ í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸° (vocab_size): {len(tokenizer)}")

# 1. EOS í† í° ì„¤ì •
if tokenizer.eos_token is None:
    print("âš ï¸ ê²½ê³ : í† í¬ë‚˜ì´ì €ì— ê¸°ë³¸ EOS í† í°ì´ ì—†ìŠµë‹ˆë‹¤. '</s>'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
    tokenizer.add_special_tokens({'eos_token': '</s>'})

# 2. PAD í† í° ì„¤ì •
if tokenizer.pad_token is None:
    print("í† í¬ë‚˜ì´ì €ì— PAD í† í°ì´ ì •ì˜ë˜ì–´ ìˆì§€ ì•Šì•„ EOS í† í°ì„ PAD í† í°ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    tokenizer.pad_token = tokenizer.eos_token

print(f"ì‚¬ìš©ë  EOS í† í°: '{tokenizer.eos_token}', ID: {tokenizer.eos_token_id}")
print(f"ì‚¬ìš©ë  BOS í† í°: '{tokenizer.bos_token}', ID: {tokenizer.bos_token_id}")
print(f"ì‚¬ìš©ë  PAD í† í°: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}")
print(f"ìµœì¢… í† í¬ë‚˜ì´ì € ì–´íœ˜ í¬ê¸° (vocab_size): {len(tokenizer)}")


# ğŸš€ ê° ë¬¸ì„œ ëì— EOS í† í° ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
def append_eos_to_text(examples):
    processed_texts = [text + tokenizer.eos_token for text in examples['text']]
    examples['text'] = processed_texts
    return examples

print("\nê° ë¬¸ì„œì— EOS í† í° ì¶”ê°€ ì¤‘...")
ds_with_eos = ds.map(
    append_eos_to_text,
    batched=True,
    batch_size=5_000, # ì‹¤ì œ ë°ì´í„°ì…‹ì— ë§ëŠ” ë°°ì¹˜ í¬ê¸°
    num_proc=os.cpu_count()
)
print("EOS ì¶”ê°€ í›„ ë°ì´í„°ì…‹ ìƒ˜í”Œ (text í•„ë“œë§Œ):")
print(ds_with_eos["train"][0]['text'][-100:])


# ğŸš€ ê°œì„ ëœ í† í°í™” í•¨ìˆ˜ (ì´ˆê¸° í† í°í™”)
def tokenize_for_grouping(element):
    return tokenizer(element["text"], truncation=False, return_attention_mask=False)

print("\nì´ˆê¸° í† í°í™” ì§„í–‰ ì¤‘ (í…ìŠ¤íŠ¸ -> í† í° ID ë¦¬ìŠ¤íŠ¸)...")
tokenized_dataset_for_grouping = ds_with_eos.map(
    tokenize_for_grouping,
    batched=True,
    batch_size=5_000, # ì‹¤ì œ ë°ì´í„°ì…‹ì— ë§ëŠ” ë°°ì¹˜ í¬ê¸°
    num_proc=os.cpu_count(),
    remove_columns=ds_with_eos["train"].column_names
)

# ğŸš€ í…ìŠ¤íŠ¸ ê·¸ë£¹í™” í•¨ìˆ˜ (Concatenate and Chunk)
def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])

    if total_length >= context_length:
        total_length = (total_length // context_length) * context_length
    
    result = {
        k: [t[i : i + context_length] for i in range(0, total_length, context_length)]
        for k, t in concatenated_examples.items()
    }
    return result

print("\ní† í°í™”ëœ í…ìŠ¤íŠ¸ ê·¸ë£¹í™” ì¤‘ (Concatenate and Chunk)...")
lm_datasets = tokenized_dataset_for_grouping.map(
    group_texts,
    batched=True,
    batch_size=500, # ê·¸ë£¹í™” ì‹œ ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ë”°ë¼ ì¡°ì ˆ)
    num_proc=os.cpu_count(),
)
print("ê·¸ë£¹í™”ëœ ë°ì´í„°ì…‹ êµ¬ì¡°:")
print(lm_datasets)
if len(lm_datasets["train"]) > 0:
    print("ê·¸ë£¹í™”ëœ í•™ìŠµ ë°ì´í„°ì…‹ ì²« ìƒ˜í”Œ input_ids ê¸¸ì´:", len(lm_datasets["train"][0]['input_ids']))
    print(f"ê·¸ë£¹í™” í›„ í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(lm_datasets['train'])}")
    print(f"ê·¸ë£¹í™” í›„ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(lm_datasets['test'])}")
else:
    print("ê·¸ë£¹í™”ëœ í•™ìŠµ ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. context_length ë˜ëŠ” ë°ì´í„° ì–‘ì„ í™•ì¸í•˜ì„¸ìš”.")


# ğŸš€ ëª¨ë¸ ì´ˆê¸°í™”
model_config_name = "HuggingFaceTB/SmolLM2-135M" # ì›ë˜ ëª¨ë¸ configë¡œ ë³µì›
print(f"ëª¨ë¸ ì„¤ì • ë¡œë“œ ì¤‘ ('{model_config_name}')...")
config = AutoConfig.from_pretrained(
    model_config_name,
    vocab_size=len(tokenizer),
    max_position_embeddings=context_length, # Llama ê³„ì—´ì€ ì´ í•„ë“œ ì‚¬ìš©
    # n_positions=context_length, # ì¼ë¶€ ëª¨ë¸ì€ ì´ í•„ë“œ ì‚¬ìš©
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
    # LlamaForCausalLMì€ configì˜ ë‹¤ë¥¸ ì•„í‚¤í…ì²˜ íŒŒë¼ë¯¸í„° (ì˜ˆ: hidden_size, num_layers)ë¥¼
    # SmolLM2-135Mì˜ ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜, ì—†ë‹¤ë©´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ë§Œì•½ ì•„í‚¤í…ì²˜ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ, í•´ë‹¹ íŒŒë¼ë¯¸í„°ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ configì— ëª…ì‹œí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
)

print(f"LlamaForCausalLM ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ (vocab_size={config.vocab_size}, max_pos_emb={config.max_position_embeddings})...")
model = LlamaForCausalLM(config)

model_size = sum(t.numel() for t in model.parameters())
print(f"\nëª¨ë¸ í¬ê¸°: {model_size/1000**2:.1f}M parameters")


# ğŸš€ í‰ê°€ ì§€í‘œ ì¤€ë¹„
accuracy_metric = evaluate.load("accuracy")

def preprocess_logits_for_metrics(logits, _):
    # 'labels' íŒŒë¼ë¯¸í„°ê°€ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ '_'ë¡œ ë³€ê²½
    if isinstance(logits, tuple): # ëª¨ë¸ ì¶œë ¥ì—ì„œ logits ì¶”ì¶œ
        return logits[0]
    return logits

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    
    # Perplexity ê³„ì‚°
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    perplexity = math.exp(loss.item())

    # Accuracy ê³„ì‚°
    predictions = torch.argmax(logits, axis=-1)
    labels_for_acc = labels[:, 1:].reshape(-1)
    preds_for_acc = predictions[:, :-1].reshape(-1)
    mask = labels_for_acc != tokenizer.pad_token_id # DataCollatorê°€ pad_token_idë¥¼ -100ìœ¼ë¡œ ë°”ê¿€ìˆ˜ë„ ìˆìŒ. ì¼ê´€ì„± í™•ì¸.
                                                    # TrainerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ labelì˜ -100ì„ ë¬´ì‹œí•©ë‹ˆë‹¤.
                                                    # DataCollatorForLanguageModelingì˜ ê¸°ë³¸ ignore_indexê°€ -100 ì´ë¯€ë¡œ,
                                                    # tokenizer.pad_token_id ëŒ€ì‹  -100ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¼ë°˜ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                                                    # ì—¬ê¸°ì„œëŠ” tokenizer.pad_token_idë¡œ ì„¤ì •ëœ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤.
                                                    # ë§Œì•½ pad_token_idê°€ -100ì´ ì•„ë‹ˆë¼ë©´, loss_fctì˜ ignore_indexë„ ë§ì¶°ì•¼í•©ë‹ˆë‹¤.

    # DataCollatorForLanguageModelingì€ ê¸°ë³¸ì ìœ¼ë¡œ labelsì˜ padding ë¶€ë¶„ì„ -100ìœ¼ë¡œ ì±„ì›€.
    # ë”°ë¼ì„œ maskëŠ” labels_for_acc != -100 ì´ ë” ì•ˆì „í•  ìˆ˜ ìˆìŒ.
    actual_mask_value = -100 # HF Trainer/DataCollator í‘œì¤€
    mask = labels_for_acc != actual_mask_value

    valid_labels = labels_for_acc[mask]
    valid_preds = preds_for_acc[mask]
    accuracy = accuracy_metric.compute(predictions=valid_preds, references=valid_labels)["accuracy"]
    
    return {
        "perplexity": perplexity,
        "accuracy": accuracy,
        "eval_loss": loss.item()
    }


# ğŸš€ ë°ì´í„° ì½œë ˆì´í„°
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
print(f"ë°ì´í„° ì½œë ˆì´í„° pad_token_id: {tokenizer.pad_token_id} (ì‹¤ì œë¡œëŠ” -100ìœ¼ë¡œ ì±„ì›Œì§ˆ ìˆ˜ ìˆìŒ)")


# ğŸš€ TrainingArguments
output_dir_name = "model/pretrained-tiny-ko-eos" # ì‹¤ì œ í”„ë¡œì íŠ¸ì— ë§ê²Œ ìˆ˜ì •
hub_id = "minpeter/pretrain-tiny-ko-eos" # ì‹¤ì œ Hugging Face Hub IDë¡œ ë³€ê²½

# í•™ìŠµ ì „ í† í¬ë‚˜ì´ì € ì €ì¥
tokenizer.save_pretrained(output_dir_name)
tokenizer.push_to_hub(hub_id) # ì‹¤ì œ Hubì— ì—…ë¡œë“œ ì‹œ í™œì„±í™”
print(f"í† í¬ë‚˜ì´ì €ê°€ '{output_dir_name}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

args = TrainingArguments(
    output_dir=output_dir_name,

    push_to_hub=True, # ì‹¤ì œ Hubì— ì—…ë¡œë“œ ì‹œ í™œì„±í™”
    hub_model_id=hub_id, # ì‹¤ì œ Hubì— ì—…ë¡œë“œ ì‹œ í™œì„±í™”
    hub_strategy="every_save",

    eval_strategy="steps",
    save_strategy="steps",
    eval_steps=1000,       # ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
    save_steps=1000,       # ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›

    gradient_accumulation_steps=4, # ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
    per_device_train_batch_size=56, # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì • (ì›ë˜ 56ì€ ë§¤ìš° í´ ìˆ˜ ìˆìŒ, A100 40GB ê¸°ì¤€ 8~16 ì ì ˆ)
    per_device_eval_batch_size=56,  # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì¡°ì •

    logging_steps=5, # ë¡œê·¸ ë¹ˆë„ ì¡°ì •
    num_train_epochs=2,    # ì›ë˜ ê°’ìœ¼ë¡œ ë³µì› (ë˜ëŠ” í•„ìš”ì— ë”°ë¼ ì¡°ì ˆ)
    weight_decay=0.1,
    warmup_steps=200,     # ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
    lr_scheduler_type="cosine",
    learning_rate=1e-4,    # ì›ë˜ ê°’ìœ¼ë¡œ ë³µì›
    
    bf16=torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8, # Ampere ì´ìƒì—ì„œ BF16 ì‚¬ìš©
    fp16=not (torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8) and torch.cuda.is_available(), # BF16 ì‚¬ìš© ë¶ˆê°€ ì‹œ FP16

    save_total_limit=5,
    load_best_model_at_end=True,
    metric_for_best_model="eval_perplexity",
    greater_is_better=False,

    # torch_compile=True, # PyTorch 2.0+ ë° í˜¸í™˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ (ì²« ìŠ¤í…ì´ ëŠë¦´ ìˆ˜ ìˆìŒ)
    # report_to="tensorboard",
    # deepspeed="ds_config.json", # DeepSpeed ì‚¬ìš© ì‹œ ì„¤ì • íŒŒì¼ ê²½ë¡œ
)

trainer = Trainer(
    model=model,
    args=args,
    data_collator=data_collator,
    train_dataset=lm_datasets["train"] if lm_datasets and len(lm_datasets["train"]) > 0 else None,
    eval_dataset=lm_datasets["test"] if lm_datasets and len(lm_datasets["test"]) > 0 else None,
    compute_metrics=compute_metrics,
    preprocess_logits_for_metrics=preprocess_logits_for_metrics,
)

print("\ní›ˆë ¨ ì‹œì‘ ì¤€ë¹„ ì™„ë£Œ!")

if trainer.train_dataset and trainer.eval_dataset:
    print(f"ì´ í•™ìŠµ ìŠ¤í… ìˆ˜ ì˜ˆìƒ: { (len(lm_datasets['train']) // (args.per_device_train_batch_size * args.gradient_accumulation_steps * max(1, args.world_size))) * args.num_train_epochs }")
    # print("ì‹¤ì œ í•™ìŠµì„ ì‹œì‘í•˜ë ¤ë©´ ì•„ë˜ trainer.train() ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.")
    trainer.train()
    # print("ì‹¤ì œ í•™ìŠµì€ ì£¼ì„ ì²˜ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
else:
    print("í•™ìŠµ ë˜ëŠ” í‰ê°€ ë°ì´í„°ì…‹ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ í›ˆë ¨ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ë°ì´í„°ì…‹ ë¡œë“œ, í† í°í™”, ê·¸ë£¹í™” ê³¼ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    if lm_datasets:
        print(f"í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(lm_datasets.get('train', []))}")
        print(f"í‰ê°€ ë°ì´í„° ìƒ˜í”Œ ìˆ˜: {len(lm_datasets.get('test', []))}")