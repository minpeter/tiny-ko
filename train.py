# RUN COMMAND: time uv run accelerate launch train.py

import os
import evaluate
import torch
from transformers import (
    AutoTokenizer,
    LlamaForCausalLM,
    AutoConfig,
    LlamaConfig,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
)
from datasets import load_dataset, concatenate_datasets

ds_kr = load_dataset("minpeter/tiny-ko-corpus", split="train")

# >>> en dataset >>>
cosmopedia = load_dataset(
    "HuggingFaceTB/smollm-corpus",
    data_files=[f"cosmopedia-v2/train-{i:05d}-of-00104.parquet" for i in range(21)],
    split="train",
)
fineweb = load_dataset(
    "HuggingFaceTB/smollm-corpus",
    data_files=[f"fineweb-edu-dedup/train-{i:05d}-of-00234.parquet" for i in range(21)],
    split="train",
)
cosmopedia_text = cosmopedia.remove_columns(
    [col for col in cosmopedia.column_names if col != "text"]
)
fineweb_text = fineweb.remove_columns(
    [col for col in fineweb.column_names if col != "text"]
)
ds_en = concatenate_datasets([cosmopedia_text, fineweb_text])
# <<< en dataset <<<

ds = concatenate_datasets([ds_kr, ds_en])

ds = ds.train_test_split(test_size=0.001, shuffle=True, seed=5768112)
print(ds)

context_length = 8192
max_cpu_count = int(os.cpu_count() / 3) or 1

tokenizer = AutoTokenizer.from_pretrained("./tknz/tiny-ko-tokenizer")

try:
    print(f"ì‚¬ìš©ë  EOS í† í°: '{tokenizer.eos_token}', ID: {tokenizer.eos_token_id}")
    print(f"ì‚¬ìš©ë  PAD í† í°: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}")
    print(f"ì‚¬ìš©ë  BOS í† í°: '{tokenizer.bos_token}', ID: {tokenizer.bos_token_id}")
except AttributeError as e:
    print(e)


def append_eos_to_text(examples):
    processed_texts = [text + tokenizer.eos_token for text in examples["text"]]
    examples["text"] = processed_texts
    return examples


print("\nê° ë¬¸ì„œì— EOS í† í° ì¶”ê°€ ì¤‘...")
ds_with_eos = ds.map(
    append_eos_to_text, batched=True, batch_size=5_000, num_proc=max_cpu_count
)
print("EOS ì¶”ê°€ í›„ ë°ì´í„°ì…‹ ìƒ˜í”Œ (text í•„ë“œë§Œ):")
print(ds_with_eos["train"][0]["text"][-100:])


def tokenize(element):
    outputs = tokenizer(
        element["text"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    return outputs


print("\ní† í°í™” ì§„í–‰ ì¤‘...")
tokenized_dataset = ds_with_eos.map(
    tokenize,
    remove_columns=ds_with_eos["train"].column_names,
    batched=True,
    batch_size=5_000,
    num_proc=max_cpu_count,
)
print("í† í°í™”ëœ ë°ì´í„°ì…‹ êµ¬ì¡°:")
print(tokenized_dataset)
print("í† í°í™”ëœ ë°ì´í„°ì…‹ ìƒ˜í”Œ (input_ids):")
for i in range(min(5, len(tokenized_dataset["train"]))):
    last_tokens = tokenized_dataset["train"][i]["input_ids"][-5:]
    print(
        f"ìƒ˜í”Œ {i}ì˜ ë§ˆì§€ë§‰ 5ê°œ í† í° ID: {last_tokens}, EOS IDì™€ ë¹„êµ: {tokenizer.eos_token_id}"
    )
    if tokenizer.eos_token_id in last_tokens:
        print(
            f"  ìƒ˜í”Œ {i}ì˜ ë§ˆì§€ë§‰ì— EOS í† í° ID({tokenizer.eos_token_id})ê°€ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤."
        )


# ðŸš€ ëª¨ë¸ ì´ˆê¸°í™” (vocab_sizeëŠ” í† í¬ë‚˜ì´ì € ê¸¸ì´ì— ë§žì¶¤)
# config = AutoConfig.from_pretrained(
#     "HuggingFaceTB/SmolLM2-135M",
#     vocab_size=len(tokenizer),    # EOS ë˜ëŠ” ë‹¤ë¥¸ í† í° ì¶”ê°€ë¡œ ì¸í•´ tokenizer ê¸¸ì´ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìžˆìŒ
#     max_position_embeddings=context_length,
#     # bos_token_id=tokenizer.bos_token_id,
#     eos_token_id=tokenizer.eos_token_id, # ìœ„ì—ì„œ ì„¤ì •ëœ tokenizer.eos_token_id ì‚¬ìš©
#     pad_token_id=tokenizer.pad_token_id, # ìœ„ì—ì„œ ì„¤ì •ëœ tokenizer.pad_token_id ì‚¬ìš© (eos_token_idì™€ ê°™ì„ ìˆ˜ ìžˆìŒ)
# )
config = LlamaConfig(
    hidden_size=256,
    num_hidden_layers=12,
    intermediate_size=1024,
    tie_word_embeddings=True,
    num_attention_heads=4,
    num_key_value_heads=2,
    vocab_size=len(tokenizer),
    max_position_embeddings=context_length,
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
)


config._attn_implementation = "flash_attention_2"
model = LlamaForCausalLM(config)
model = model.to(torch.bfloat16)

model_size = sum(t.numel() for t in model.parameters())
print(f"\nëª¨ë¸ í¬ê¸°: {model_size/1000**3:.2f}B parameters")


def preprocess_logits_for_metrics(logits, labels):
    if isinstance(logits, tuple):
        logits = logits[0]
    return torch.argmax(logits, axis=-1)


metric = evaluate.load("accuracy")


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # preds have the same shape as the labels, after the argmax(-1) has been calculated
    # by preprocess_logits_for_metrics but we need to shift the labels
    labels = labels[:, 1:].reshape(-1)
    preds = preds[:, :-1].reshape(-1)
    mask = labels != -100
    labels = labels[mask]
    preds = preds[mask]
    return metric.compute(predictions=preds, references=labels)


data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

hf_model_id = "minpeter/tiny-ko-20m-base-en"
local_model_path = "model/tiny-ko-20m-base-en"

tokenizer.save_pretrained(local_model_path)
tokenizer.push_to_hub(hf_model_id)

args = TrainingArguments(
    output_dir=local_model_path,
    push_to_hub=True,
    hub_model_id=hf_model_id,
    hub_strategy="every_save",
    eval_strategy="steps",
    save_strategy="steps",
    eval_steps=1_000,
    save_steps=1_000,
    gradient_accumulation_steps=4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_steps=5,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    bf16=True,
    torch_compile=True,
    dataloader_num_workers=max_cpu_count,
    save_total_limit=5,
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",
    greater_is_better=True,
)

trainer = Trainer(
    model=model,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
    preprocess_logits_for_metrics=preprocess_logits_for_metrics,
)

trainer.train()
