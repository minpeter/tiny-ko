base_model: ./tiny-random
# Automatically upload checkpoint and final model to HF
# hub_model_id: username/custom_model_name

pretraining_dataset:
  - path: wikimedia/wikipedia
    name: 20231101.ko
    text_column: text
    type: pretrain

val_set_size: 0.04

max_steps: 648000
save_steps: 1000
warmup_steps: 100
eval_steps: 2000

dataset_prepared_path: last_run_prepared
val_set_size: 0.05
output_dir: ./outputs/out

sequence_len: 2048
sample_packing: true
pad_to_sequence_len: true

hub_strategy: every_save
hub_model_id: minpeter/pretrained-tiny-ko

wandb_project: "axolotl"
wandb_entity: "kasfiekfs-e"
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 8
micro_batch_size: 4

optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 1e-3

bf16: auto
tf32: false

# gradient_checkpointing: true
# gradient_checkpointing_kwargs:
#   use_reentrant: false
resume_from_checkpoint:
logging_steps: 1
flash_attention: true

weight_decay: 0.0
# special_tokens:
#   pad_token: "<|finetune_right_pad_id|>"

fsdp:
  - full_shard
fsdp_config:
  sharding_strategy: FULL_SHARD
  min_num_params: 100000000
  mixed_precision: true
  backward_prefetch: BACKWARD_PRE
  limit_all_gathers: true
  cpu_offload: false
  sync_module_states: true
  auto_wrap_policy: TRANSFORMER_BASED_WRAP
  use_orig_params: true
