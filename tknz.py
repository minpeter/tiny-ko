# -----------------------------------------------------------------------------
# STEP 0: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (transformers ì¶”ê°€)
# -----------------------------------------------------------------------------
# í„°ë¯¸ë„ì—ì„œ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”:
# pip install datasets tokenizers transformers
# -----------------------------------------------------------------------------

import os
from datasets import load_dataset
from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors
from transformers import PreTrainedTokenizerFast, AutoTokenizer # ğŸ”„ [ì¶”ê°€] AutoTokenizer ë¡œë”©ì„ ìœ„í•œ í´ë˜ìŠ¤ ì„í¬íŠ¸

def train_and_save_huggingface_tokenizer():
    """
    AutoTokenizer í˜¸í™˜ Fast í† í¬ë‚˜ì´ì €ë¥¼ í•™ìŠµí•˜ê³  ì €ì¥í•˜ëŠ” ìµœì¢… í•¨ìˆ˜
    """
    
    # -----------------------------------------------------------------------------
    # STEP 1 ~ 3: ì´ì „ê³¼ ë™ì¼
    # -----------------------------------------------------------------------------
    dataset = load_dataset("minpeter/tiny-ko-corpus", split='train[:1000]')
    
    print("âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ")
    print(dataset)

    def get_training_corpus():
        for i in range(len(dataset)):
             # None íƒ€ì…ì´ ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
            text = dataset[i]['text']
            yield text if text is not None else ""
            
    tokenizer = Tokenizer(models.BPE(unk_token="[UNK]"))
    tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC(), normalizers.Lowercase()])
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    tokenizer.decoder = decoders.ByteLevel()

    vocab_size = 32000
    special_tokens = ["[UNK]", "[PAD]", "[BOS]", "[EOS]", "<|endoftext|>"]
    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=special_tokens)
    
    print("â³ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer, length=len(dataset))
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    
    # -----------------------------------------------------------------------------
    # ğŸ”„ [ìˆ˜ì •] STEP 4, 5 í†µí•©: AutoTokenizer í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ì €ì¥
    # -----------------------------------------------------------------------------
    print("\nâœ… AutoTokenizer í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•©ë‹ˆë‹¤...")

    # 4-1. í›ˆë ¨ëœ tokenizer ê°ì²´ë¥¼ PreTrainedTokenizerFast ë¡œ ë˜í•‘(wrapping)
    # ì´ ê³¼ì •ì—ì„œ special tokenë“¤ì˜ ì—­í• ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•´ì¤ë‹ˆë‹¤.
    special_tokens_map = {
        "unk_token": "[UNK]",
        "pad_token": "[PAD]",
        "bos_token": "[BOS]",
        "eos_token": "[EOS]",
    }
    
    fast_tokenizer_wrapper = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer, # í›ˆë ¨ëœ tokenizer ê°ì²´ë¥¼ ì „ë‹¬
        **special_tokens_map
    )

    # 4-2. ì €ì¥í•  ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = "./tknz/my_llm_tokenizer_for_hf"
    os.makedirs(output_dir, exist_ok=True)
    
    # 4-3. .save_pretrained() í˜¸ì¶œ: ëª¨ë“  í•„ìš” íŒŒì¼(json)ì„ ìë™ìœ¼ë¡œ ìƒì„±!
    fast_tokenizer_wrapper.save_pretrained(output_dir)
    
    print(f"âœ… í† í¬ë‚˜ì´ì €ê°€ '{output_dir}' ê²½ë¡œì— ëª¨ë“  ì„¤ì • íŒŒì¼ê³¼ í•¨ê»˜ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ìƒì„±ëœ íŒŒì¼ ëª©ë¡:", os.listdir(output_dir))

    # -----------------------------------------------------------------------------
    # STEP 6: ìµœì¢… ê²€ì¦ (AutoTokenizerë¡œ ì§ì ‘ ë¡œë“œí•´ë³´ê¸°)
    # -----------------------------------------------------------------------------
    print("\n--- ìµœì¢… ê²€ì¦ ---")
    print(f"'{output_dir}' ê²½ë¡œì—ì„œ AutoTokenizerë¡œ ë¡œë”©ì„ ì‹œë„í•©ë‹ˆë‹¤...")
    
    try:
        loaded_tokenizer_hf = AutoTokenizer.from_pretrained(output_dir)
        print("âœ… AutoTokenizer ë¡œë”© ì„±ê³µ!")
        
        text_to_test = "ì´ë ‡ê²Œ ë§Œë“  LLMìš© í† í¬ë‚˜ì´ì €, ì˜ ë ê¹Œ?"
        output = loaded_tokenizer_hf(text_to_test) # __call__ ë©”ì†Œë“œë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

        print(f"\ní…ŒìŠ¤íŠ¸ ë¬¸ì¥: {text_to_test}")
        print(f"ì¸ì½”ë”©ëœ ID: {output['input_ids']}")
        print(f"ì–´í…ì…˜ ë§ˆìŠ¤í¬: {output['attention_mask']}")
        
        decoded_text = loaded_tokenizer_hf.decode(output['input_ids'])
        print(f"ë””ì½”ë”©ëœ ë¬¸ì¥: {decoded_text}")
    except Exception as e:
        print(f"âŒ AutoTokenizer ë¡œë”© ì‹¤íŒ¨: {e}")

# ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
if __name__ == "__main__":
    train_and_save_huggingface_tokenizer()